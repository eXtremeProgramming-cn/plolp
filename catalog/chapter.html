
<!DOCTYPE HTML>
<html lang="" >
    <head>
        <meta charset="UTF-8">
        <meta content="text/html; charset=utf-8" http-equiv="Content-Type">
        <title>Chapter Â· GitBook</title>
        <meta http-equiv="X-UA-Compatible" content="IE=edge" />
        <meta name="description" content="">
        <meta name="generator" content="GitBook 3.2.3">
        
        
        
    
    <link rel="stylesheet" href="../gitbook/style.css">

    
            
                
                <link rel="stylesheet" href="../gitbook/gitbook-plugin-highlight/website.css">
                
            
                
                <link rel="stylesheet" href="../gitbook/gitbook-plugin-search/search.css">
                
            
                
                <link rel="stylesheet" href="../gitbook/gitbook-plugin-fontsettings/website.css">
                
            
        

    

    
        
    
        
    
        
    
        
    
        
    
        
    

        
    
    
    <meta name="HandheldFriendly" content="true"/>
    <meta name="viewport" content="width=device-width, initial-scale=1, user-scalable=no">
    <meta name="apple-mobile-web-app-capable" content="yes">
    <meta name="apple-mobile-web-app-status-bar-style" content="black">
    <link rel="apple-touch-icon-precomposed" sizes="152x152" href="../gitbook/images/apple-touch-icon-precomposed-152.png">
    <link rel="shortcut icon" href="../gitbook/images/favicon.ico" type="image/x-icon">

    
    <link rel="next" href="problem_orientation.html" />
    
    
    <link rel="prev" href="prompt_file.html" />
    

    </head>
    <body>
        
<div class="book">
    <div class="book-summary">
        
            
<div id="book-search-input" role="search">
    <input type="text" placeholder="Type to search" />
</div>

            
                <nav role="navigation">
                


<ul class="summary">
    
    

    

    
        
        <li class="header">Opening</li>
        
        
    
        <li class="chapter " data-level="1.1" data-path="../">
            
                <a href="../">
            
                    
                    Introduction
            
                </a>
            

            
        </li>
    

    
        
        <li class="header">Organizing Prompts</li>
        
        
    
        <li class="chapter " data-level="2.1" data-path="long_prompt.html">
            
                <a href="long_prompt.html">
            
                    
                    Long Prompt
            
                </a>
            

            
        </li>
    
        <li class="chapter " data-level="2.2" data-path="prompt_file.html">
            
                <a href="prompt_file.html">
            
                    
                    Prompt File
            
                </a>
            

            
        </li>
    
        <li class="chapter active" data-level="2.3" data-path="chapter.html">
            
                <a href="chapter.html">
            
                    
                    Chapter
            
                </a>
            

            
        </li>
    

    
        
        <li class="header">Defining Tasks</li>
        
        
    
        <li class="chapter " data-level="3.1" data-path="problem_orientation.html">
            
                <a href="problem_orientation.html">
            
                    
                    Problem Orientation
            
                </a>
            

            
        </li>
    
        <li class="chapter " data-level="3.2" data-path="prescribed_process.html">
            
                <a href="prescribed_process.html">
            
                    
                    Prescribed Process
            
                </a>
            

            
        </li>
    
        <li class="chapter " data-level="3.3" data-path="example.html">
            
                <a href="example.html">
            
                    
                    Example
            
                </a>
            

            
        </li>
    

    
        
        <li class="header">Setting Context</li>
        
        
    
        <li class="chapter " data-level="4.1" data-path="persona.html">
            
                <a href="persona.html">
            
                    
                    Persona Adoption
            
                </a>
            

            
        </li>
    
        <li class="chapter " data-level="4.2" data-path="audience.html">
            
                <a href="audience.html">
            
                    
                    Audience
            
                </a>
            

            
        </li>
    
        <li class="chapter " data-level="4.3" data-path="epistemology_frame.html">
            
                <a href="epistemology_frame.html">
            
                    
                    Epistemology Frame
            
                </a>
            

            
        </li>
    
        <li class="chapter " data-level="4.4" data-path="glossary.html">
            
                <a href="glossary.html">
            
                    
                    Glossary
            
                </a>
            

            
        </li>
    

    
        
        <li class="header">Structuring Input</li>
        
        
    
        <li class="chapter " data-level="5.1" data-path="exhaustive_input.html">
            
                <a href="exhaustive_input.html">
            
                    
                    Exhaustive Input
            
                </a>
            

            
        </li>
    
        <li class="chapter " data-level="5.2" data-path="online_search.html">
            
                <a href="online_search.html">
            
                    
                    Online Search
            
                </a>
            

            
        </li>
    
        <li class="chapter " data-level="5.3" data-path="attachment.html">
            
                <a href="attachment.html">
            
                    
                    Attachment
            
                </a>
            

            
        </li>
    
        <li class="chapter " data-level="5.4" data-path="relevance.html">
            
                <a href="relevance.html">
            
                    
                    Relevance
            
                </a>
            

            
        </li>
    

    
        
        <li class="header">Shaping Output</li>
        
        
    
        <li class="chapter " data-level="6.1" data-path="structured_output.html">
            
                <a href="structured_output.html">
            
                    
                    Structured Output
            
                </a>
            

            
        </li>
    
        <li class="chapter " data-level="6.2" data-path="style_mimicry.html">
            
                <a href="style_mimicry.html">
            
                    
                    Style Mimicry
            
                </a>
            

            
        </li>
    
        <li class="chapter " data-level="6.3" data-path="deconstruction.html">
            
                <a href="deconstruction.html">
            
                    
                    Deconstruction
            
                </a>
            

            
        </li>
    

    
        
        <li class="header">Sharpening Your Mind</li>
        
        
    
        <li class="chapter " data-level="7.1" data-path="never_stop_winning.html">
            
                <a href="never_stop_winning.html">
            
                    
                    Never Stop Winning
            
                </a>
            

            
        </li>
    
        <li class="chapter " data-level="7.2" data-path="iteration.html">
            
                <a href="iteration.html">
            
                    
                    Iteration
            
                </a>
            

            
        </li>
    
        <li class="chapter " data-level="7.3" data-path="decomposition.html">
            
                <a href="decomposition.html">
            
                    
                    Decomposition
            
                </a>
            

            
        </li>
    
        <li class="chapter " data-level="7.4" data-path="precision.html">
            
                <a href="precision.html">
            
                    
                    Precision
            
                </a>
            

            
        </li>
    
        <li class="chapter " data-level="7.5" data-path="brainstorm.html">
            
                <a href="brainstorm.html">
            
                    
                    Brainstorm
            
                </a>
            

            
        </li>
    
        <li class="chapter " data-level="7.6" data-path="criticism.html">
            
                <a href="criticism.html">
            
                    
                    Criticism
            
                </a>
            

            
        </li>
    

    

    <li class="divider"></li>

    <li>
        <a href="https://www.gitbook.com" target="blank" class="gitbook-link">
            Published with GitBook
        </a>
    </li>
</ul>


                </nav>
            
        
    </div>

    <div class="book-body">
        
            <div class="body-inner">
                
                    

<div class="book-header" role="navigation">
    

    <!-- Title -->
    <h1>
        <i class="fa fa-circle-o-notch fa-spin"></i>
        <a href=".." >Chapter</a>
    </h1>
</div>




                    <div class="page-wrapper" tabindex="-1" role="main">
                        <div class="page-inner">
                            
<div id="book-search-results">
    <div class="search-noresults">
    
                                <section class="normal markdown-section">
                                
                                <h1 id="chapter">Chapter</h1>
<p><em>Organize prompts into chapters for better structure and reuse.</em></p>
<h2 id="motivation">Motivation</h2>
<p>For lengthy or multi-part instructions, presenting everything in one big block can overwhelm both the LLM and you as the author. Without clear divisions (chapters or sections), the prompt may lack a logical flow, making it difficult for the model to follow and for you to maintain or refine later. Important context or instructions might be buried or overlooked when the structure isn&#x2019;t clear, and you could even end up inadvertently repeating yourself without realizing it.</p>
<h2 id="solution">Solution</h2>
<p>Divide your prompt into multiple titled sections or &#x201C;chapters,&#x201D; each focusing on a specific aspect of the task. This structure provides a clear roadmap for the LLM to follow, ensuring it processes information in a logical order. It also helps you manage the content. You can update or reuse individual chapters (or even attach separate documents as chapters) instead of dealing with one large, monolithic prompt.</p>
<h2 id="challenge">Challenge</h2>
<p>Many LLM users initially approach prompting by pouring everything they want to say into one long, continuous message. They might think that giving all the details in a single prompt will make the AI understand better. It&#x2019;s an understandable impulse &#x2013; after all, you have a lot of information and guidance to convey, so why not just write it all out? The problem is that writing a prompt without any structure quickly leads to confusion and subpar results. If you were writing a report or an email with multiple topics, you wouldn&apos;t cram it all into one giant paragraph; you&apos;d break it into sections or at least separate paragraphs so the reader can follow. Likewise, when you present a huge block of text to an LLM with no clear breaks or hierarchy, both you and the AI can lose track of what&#x2019;s what.</p>
<p>In practice, an unstructured prompt often looks like a big run-on request, covering everything at once. For example, imagine Alice, a high school teacher who wants an LLM to help create a lesson plan on climate change. Alice types out a single, lengthy prompt that includes the lesson topic, some background about her class, the specific points to cover (causes, effects, and solutions of climate change), as well as extra directives like making the lesson engaging, including two interactive activities, adding a visual demonstration, providing a summary for the students, and even writing a short quiz at the end. She basically dumps all of these requirements into one paragraph, hoping the AI will catch every detail. </p>
<p>Alice has certainly communicated all the pieces of her request, but they&#x2019;re all tangled together. What kind of response does this prompt get? Possibly something usable, but not exactly what she needs. Let&apos;s say the AI produces an answer: it might outline a basic lesson on climate change, but perhaps it forgot to include the quiz, or it included only one activity instead of two. Maybe the output is just a long blob of text that isn&apos;t clearly separated into a student handout versus the main lesson plan. In other words, because Alice&apos;s instructions were all merged in one place, the AI might overlook some of them or give them uneven attention. The result is mediocre and not quite what she was hoping for.</p>
<p>Now Alice faces the challenge of fixing her prompt. She knows the AI missed the quiz and that the format isn&#x2019;t what she wanted, but how can she tweak her request? In a single huge prompt, it&apos;s hard to tell where to make changes. Should she repeat the quiz instruction again to make it more obvious? Should she re-order her sentences? It&apos;s not clear. There&#x2019;s no easy way for her to pinpoint why the model ignored something. This is a common situation when prompts are unstructured: if the output isn&apos;t correct, you end up guessing what to add or change, often by trial and error, because you lack a systematic way to improve the prompt.</p>
<p>For a moment, put yourself in Alice&#x2019;s shoes &#x2013; or recall a time you tried something similar. An unstructured prompt like this presents several specific challenges:</p>
<ul>
<li><p><strong>Important details get lost:</strong> In a big unorganized paragraph, it&apos;s easy to accidentally skip over a key detail or bury it among other information. You might also find yourself mentioning some things more than once without realizing, while other points are only mentioned briefly. In Alice&apos;s case, the requirement to include a quiz was listed at the very end of a long prompt; it&apos;s no surprise that the model might not give it as much attention. Similarly, she mentioned &quot;engaging&quot; and also &quot;interactive activities&quot; &#x2013; two related ideas that ended up scattered in the text, possibly causing confusion or redundancy. Without clear separation, neither the AI nor the person writing the prompt can easily see if every requirement has been addressed exactly once and in the right place.</p>
</li>
<li><p><strong>Hard to troubleshoot issues:</strong> When the AI&apos;s output isn&apos;t what you wanted, a monolithic prompt gives you no clues about what went wrong. Was a particular instruction unclear? Did the model gloss over it because it was hidden in the middle? With everything blended together, you can&apos;t isolate the cause. For Alice, figuring out why the AI skipped the quiz section is difficult. Did she phrase that request poorly, or did it get lost because it was tacked onto an already long sentence? There&apos;s no straightforward way to identify which part of her prompt the model misunderstood or ignored, because all the instructions are entangled. It&#x2019;s like trying to figure out which ingredient in a soup is responsible for the flavor after you&#x2019;ve thrown everything in at once &#x2013; you can&#x2019;t easily tell which component had which effect.</p>
</li>
<li><p><strong>Difficult to read and collaborate:</strong> A long, rambling prompt is not only tough for the AI, but it&apos;s also hard for humans to parse. Imagine Alice shows her prompt to a colleague or friend to get feedback. The colleague is faced with a wall of text containing all of Alice&apos;s thoughts at once. They might miss something important buried in the middle, or misinterpret her intentions because there&apos;s no clear labeling of sections. Even Alice herself, if she comes back to her prompt the next day, might have to carefully reread the whole thing to recall what she included. Without structure, discussing or refining the prompt becomes cumbersome &#x2013; there&apos;s no easy reference point like &quot;fix the part about the quiz&quot; or &quot;maybe expand the background section,&quot; because there&#x2019;s no labeled <strong>Quiz</strong> part or <strong>Background</strong> section to point to.</p>
</li>
<li><p><strong>No modularity or reuse:</strong> An unstructured prompt is an all-or-nothing block, which makes it a pain to reuse or adapt. Suppose the lesson plan prompt <em>almost</em> works and next week Alice wants a lesson plan on a different topic (say, recycling). With her current approach, she might copy that huge prompt and then manually edit it to change the topic and some details. This process is error-prone &#x2013; she could easily forget to remove or modify a detail about climate change and accidentally include it in the recycling lesson prompt. Or consider if another teacher asks Alice how she got the AI to write a lesson plan; sharing her prompt is awkward because it&apos;s just one big blob of text. Without clear parts, she can&apos;t say &quot;oh, just change this section for your subject&quot; &#x2013; because there are no sections! The lack of modular structure means every new prompt starts from scratch or requires careful surgery to modify, which is time-consuming and invites mistakes.</p>
</li>
</ul>
<p>These challenges are not trivial, especially for users who aren&apos;t tech-savvy. A technically-minded person might instinctively try to structure information (like a programmer breaking a program into functions), but an ordinary user like Alice has never had to think this way about a written request. She might assume that as long as she writes down everything she can think of, the AI will figure it out. In reality, the AI needs a clear, organized presentation of the task. Unlike a human collaborator, the AI won&#x2019;t ask clarifying questions or reorganize your instructions on its own &#x2013; it will just do its best with what you gave it, warts and all. So when the prompt is a jumbled stream of consciousness, the output will reflect that jumble.</p>
<p>Furthermore, when something goes wrong in the output, non-technical users often feel stuck. There&apos;s no obvious error message or hint to guide them. The AI&apos;s response might be missing pieces, but it won&apos;t tell you <em>why</em>. This can be frustrating: you know the information was in your prompt somewhere, so why didn&apos;t the model do it right? The concept that <strong>how</strong> you present the information matters as much as <strong>what</strong> information you present isn&apos;t immediately obvious. Many users learn the hard way that a clear structure is key to getting a good result from an LLM. I often find that once someone introduces a bit of structure to their prompt &#x2013; even just breaking it into a few paragraphs or bullet points &#x2013; the quality of the AI&#x2019;s response improves dramatically. It&#x2019;s a bit of a mindset shift: moving from thinking of a prompt as a casual one-shot question to treating it more like a well-planned outline or specification for the task.</p>
<p>In short, writing one big unstructured prompt is like giving complex instructions in a single breath &#x2013; essential points can easily get lost, misunderstood, or jumbled together. The motivation for the Chapter pattern arises directly from these pain points. By recognizing that we need to tame the complexity in our prompts, we set the stage for a solution that makes both our job and the AI&#x2019;s job easier.</p>
<h2 id="example">Example</h2>
<p>The remedy to these issues is to introduce structure. Using the Chapter pattern, we break the prompt into clear, distinct sections, each with a specific purpose. Returning to Alice&apos;s case, let&apos;s help her reorganize her lesson plan prompt into a set of chapters. The idea is to give each aspect of the request its own &#x201C;chapter&#x201D; in the prompt, rather than mixing everything together in a single block.</p>
<p>For a complex prompt like this, Alice could divide the information into logical parts. There&#x2019;s no single correct way to partition a prompt, but a good starting point is to separate the content into categories such as: </p>
<ul>
<li><strong>Role/Perspective</strong> &#x2013; Define who or what the AI should emulate, or the viewpoint/persona to adopt.</li>
<li><strong>Context/Background</strong> &#x2013; Provide any necessary background information or context the AI should know.</li>
<li><strong>Task/Objective</strong> &#x2013; State clearly what the main goal or request is.</li>
<li><strong>Requirements/Constraints</strong> &#x2013; List specific requirements, rules, or criteria that the output must satisfy.</li>
<li><strong>Output Instructions</strong> &#x2013; Describe the desired format or structure of the output (and any supplementary outputs required).</li>
</ul>
<p>Alice doesn&#x2019;t necessarily need to use these exact labels, but these categories cover the major points she had jumbled together in her original prompt. By writing each as a separate section with a clear label, she will ensure that the AI (and she herself) can easily identify each part of the instruction. </p>
<p>For example, Alice&apos;s restructured prompt might look like this (with chapters labeled in <strong>bold</strong> for clarity):</p>
<blockquote>
<p><strong>Role:</strong> You are an experienced high school science teacher.  </p>
<p><strong>Context:</strong> You are creating a lesson for a 9th grade science class. Last week the class learned about renewable energy, so they already have some background related to this topic. The class period is 45 minutes long.  </p>
<p><strong>Task:</strong> Create a detailed lesson plan on climate change for this class. The lesson should cover the causes of climate change, its effects, and possible solutions.  </p>
<p><strong>Requirements:</strong> The lesson plan must keep students engaged throughout. Include at least two interactive activities (for example, a small group discussion or a hands-on experiment) and one visual demonstration (such as a simple experiment or a chart/visual aid) to illustrate key points. Ensure the content is age-appropriate for 9th graders and fits into the 45-minute timeframe, with approximate time allocations for each part of the lesson.  </p>
<p><strong>Output:</strong> Provide the lesson plan in a clear format with distinct sections (Introduction, Main Content, Conclusion). After outlining the lesson plan, include a one-paragraph summary that can be handed out as notes to the students. Finally, add a short quiz at the end of the plan with 5 simple questions (and their answers) to assess the students&apos; understanding.</p>
</blockquote>
<p>Now, let&apos;s break down what we did here and why it helps:</p>
<ul>
<li><p><strong>Role:</strong> We started by specifying the AI&apos;s role as an &quot;experienced high school science teacher.&quot; This sets the tone and expertise level for the response. By giving the model this persona, Alice encourages it to produce a lesson plan that a teacher might create, which helps ensure the style and depth of the content are appropriate. This chapter isn&apos;t always necessary for every prompt, but in many cases telling the model who it is or what perspective to take can guide the output significantly.</p>
</li>
<li><p><strong>Context:</strong> Next, we provided context about the class and situation. We told the AI the grade level (9th grade), the fact that the students learned about renewable energy last week, and the class duration (45 minutes). All this background is now clearly separated from the actual task instruction. In Alice&apos;s original prompt, this information was mixed in with everything else; here, it&apos;s in its own section. This means the model can absorb the context on its own terms and use it when formulating the lesson plan. For example, the AI now knows not to spend time teaching the basics of renewable energy again, and it knows to keep the content appropriate for 14-15-year-old students. If Alice wanted to reuse this prompt for a different class or a different lesson, she could adjust just the <strong>Context</strong> chapter (say, change &#x201C;9th grade&#x201D; to &#x201C;10th grade&#x201D; or update the prior knowledge) without touching the rest of the prompt. The context is modular.</p>
</li>
<li><p><strong>Task:</strong> We then clearly stated the main objective in its own chapter. <em>&#x201C;Create a detailed lesson plan on climate change for this class...&#x201D;</em> This single sentence is the core ask, isolated from background details and extra requirements. By doing this, we make sure the AI knows exactly what the primary goal is. In the unstructured version, the actual task request could get a bit lost amid the details; here it stands out on its own line. If you were reading this prompt as a person, you could easily underline that one line as &quot;This is what I need to do.&quot; The AI likewise benefits from having the request front-and-center. It&#x2019;s like giving it a headline to focus on. Everything else in the prompt supports this task, but we&#x2019;ve made sure the model first sees a crystal-clear statement of what it must accomplish.</p>
</li>
<li><p><strong>Requirements:</strong> After stating the main task, we created a separate section for all the specific requirements and constraints. Notice how in this chapter we effectively used a list (written out in sentences) to enumerate each requirement: keep students engaged, include two interactive activities, include one visual demonstration, keep content age-appropriate, fit the 45-minute schedule. Each requirement is clearly articulated and given its own space. This is far easier for the model to follow than if these were buried in the middle of a long paragraph. Now, the AI can effectively tick off each requirement as it formulates the lesson plan: <em>&#x201C;Okay, I need to add two activities &#x2013; check. A visual aid &#x2013; check. Keep it engaging &#x2013; check. Make sure it&#x2019;s for 45 minutes &#x2013; check.&#x201D;</em> It&apos;s almost like a checklist for the AI to follow. For Alice (or any user), this structure also makes it easy to verify that she included everything she wanted. If later she realizes she forgot to mention something (say, ensuring a particular key term is defined in the lesson), she can add another sentence in this <strong>Requirements</strong> section without messing up the flow of the rest of the prompt. Conversely, if one of these requirements isn&apos;t needed for a different scenario, she can remove or modify that line alone. The prompt&#x2019;s logic remains intact because each requirement is modular.</p>
</li>
<li><p><strong>Output:</strong> Finally, we separate out the instructions about the desired output format and additional deliverables (the summary and quiz) into their own chapter. This section tells the AI exactly how to package the answer. We&#x2019;re not leaving it to the model to infer how to format the lesson plan or whether to include the quiz; we explicitly state these expectations under a clear <strong>Output</strong> label. In the original prompt, the instruction for a quiz and a summary was present, but here it&apos;s nearly impossible for the model to overlook because it&apos;s a dedicated part of the prompt with its own heading. By saying <em>&#x201C;Provide the lesson plan in a clear format with distinct sections... include a summary... add a short quiz...&#x201D;</em>, we&apos;ve given a step-by-step outline of what the answer should contain and look like. The AI can use this as a guide to structure its response, likely producing the lesson plan with headings (Introduction, Main Content, Conclusion) and then clearly delineating the summary and quiz as separate parts of the answer. If something were to be off about the output format, it&apos;s now straightforward for Alice to adjust this chapter &#x2013; for example, if the summary came out too short or too generic, she could revise the <strong>Output</strong> section to say <em>&#x201C;a one-paragraph summary (5-7 sentences) highlighting the key points.&#x201D;</em> Because the format instructions are isolated, tweaking them won&#x2019;t confuse the content of the lesson itself. The format and content are now neatly decoupled.</p>
</li>
</ul>
<p>By organizing the prompt in this way, we&apos;ve directly addressed all the challenges we identified earlier:</p>
<ul>
<li><p><strong>Clarity and completeness:</strong> Every piece of information has its place. There&apos;s little chance the AI will miss the quiz now, because &#x201C;quiz&#x201D; is clearly itemized in the Output section. There&#x2019;s also less risk of Alice accidentally omitting an important element &#x2013; the structure acts like a checklist for her own planning. Each chapter of the prompt can be reviewed to ensure it&#x2019;s complete. If she had forgotten to include, say, a requirement about safety precautions for an experiment, the chapter layout makes that omission obvious (there&#x2019;s a logical spot to add a line about it under Requirements).</p>
</li>
<li><p><strong>Easier troubleshooting:</strong> If the AI&apos;s response still isn&apos;t perfect, Alice can now pinpoint where to adjust her prompt. Suppose the lesson plan came out too advanced for 9th graders; she can revisit the <strong>Context</strong> or <strong>Requirements</strong> section and add an instruction like &#x201C;use language a 14-year-old can understand.&#x201D; If the quiz questions turned out too difficult, she can specify in the <strong>Output</strong> chapter that the quiz should contain simple recall questions. Because each aspect of the prompt is isolated, she doesn&apos;t have to rewrite the whole thing or worry that clarifying one part will unintentionally muddle another part. Debugging the prompt becomes a matter of editing the relevant chapter, much like fixing one section of a document instead of rewriting the entire document.</p>
</li>
<li><p><strong>Human readability:</strong> If Alice shows this structured prompt to a colleague now, it&#x2019;s immediately clear what each part of the prompt is doing. Her colleague could say, &quot;I see you&apos;ve listed the activities required; maybe also include a safety note in the Requirements section.&quot; They can have a focused discussion: <em>Is the Context accurate? Did we list all Requirements?</em> The prompt reads almost like a mini-outline, which makes it much easier for someone else to give input or for Alice herself to revisit later. The labeled sections act as guideposts for conversation and thought. Essentially, anyone reading this prompt can understand the plan without needing Alice to explain it line by line.</p>
</li>
<li><p><strong>Reusability:</strong> Perhaps most importantly, Alice now has a template for any future lesson plan prompts. Next time she needs a lesson on a different topic, she can reuse the majority of this structured prompt. She might change the Context (if the class or prior knowledge is different) and tweak the Task and specific Requirements for the new topic, but the overall framework stays the same. Even the Output instructions (having a summary and quiz) might remain identical for each lesson she plans. This modularity means she saves time and maintains consistency across her prompts. Similarly, if her colleague wants to use an LLM to create a lesson plan, Alice can hand over her &quot;chaptered&quot; prompt. The colleague can then swap in their own context and requirements, confident that the structure is sound. In other words, the prompt design itself becomes a reusable asset.</p>
</li>
</ul>
<p>The Chapter pattern isn&apos;t limited to lesson plans, of course. Anytime you have a complex prompt with multiple pieces of information or multiple instructions, you can benefit from this approach. For example, if you were using an LLM to draft a business proposal, you might create chapters like <strong>Company Background</strong>, <strong>Project Goals</strong>, <strong>Deliverables</strong>, <strong>Timeline</strong>, and <strong>Formatting Guidelines</strong> &#x2013; each section covering a different aspect of the proposal to ensure nothing important is missed. If you were writing a story with the help of an AI, you could break your prompt into sections such as <strong>Setting</strong>, <strong>Characters</strong>, <strong>Plot Outline</strong>, and <strong>Tone/Style</strong>. The core idea is to break down the task and information logically: give the model information in labeled chunks so it knows what role each chunk plays in the overall request.</p>
<p>Different tasks will naturally suggest different chapters. The key is to ask yourself: <em>what are the distinct components of what I&apos;m asking?</em> Is there background info separate from the main instructions? Are there examples or reference data that should be isolated? Are there specific format requirements that should be stated apart from the content requirements? By thinking in terms of sections, you ensure no part of your request is stepping on another, and you make it much easier to manage each piece.</p>
<p>One useful side effect of writing in chapters is that it forces <em>you</em>, the user, to clarify your own thinking. When everything is in one blob, it&apos;s easy to be inconsistent or vague without noticing. As you separate a prompt into chapters, you naturally have to check: <em>&#x201C;Am I repeating myself? Does each section make sense on its own? Are all these instructions necessary and distinct?&#x201D;</em> It&apos;s similar to outlining an essay or planning a project &#x2013; the structure tends to expose any gaps or redundancies in your plan, allowing you to refine your prompt before the AI even sees it.</p>
<p>In practice, you&apos;ll find that LLMs respond very well to a structured prompt. Our reworked lesson plan prompt gives the model a clear roadmap: first understand the context and adopt the teacher role, then see the specific task, then consider the detailed requirements, and finally format the answer as instructed. We are effectively walking the AI through the problem step by step in the prompt itself. Modern LLMs are quite capable of handling complex, multi-part instructions, but they do so more reliably when those instructions are organized and easy to follow. Users often report that when they break a convoluted request into bullet points or sections, the resulting answers are not only more accurate but also more in line with the desired format, and the process of refining the prompt becomes smoother.</p>
<p>To summarize, the Chapter pattern helps turn a messy prompt into a well-structured one. Instead of hoping the AI will miraculously parse a tangled request correctly, you&apos;re guiding it with clear signposts. You&#x2019;re saying: &#x201C;Here&#x2019;s the background you need. Now, here&#x2019;s what I want you to do. And these are the specifics you must pay attention to. Finally, deliver the answer in this format.&#x201D; This level of clarity is beneficial to an AI which, at its core, processes text sequentially and looks for patterns &#x2013; clear separators and labels in the prompt text help it allocate attention appropriately and reduce ambiguity. For you, the prompt-writer, chapters make it feasible to handle complex instructions without being overwhelmed.</p>
<p>By organizing a prompt into chapters, we achieve both structure and reuse. <strong>Structure</strong> means your prompt has an internal order and logic that both you and the AI can follow. <strong>Reuse</strong> means you can take those structured pieces and adapt them easily to new contexts or tasks. When you revisit the prompt later, you won&apos;t have to decipher a tangled mess; you&apos;ll see a clean outline of what you were trying to accomplish.</p>
<p>At this point, you&#x2019;ve seen how a disorganized prompt can be transformed into a clear, well-structured one using the Chapter pattern. This is a foundational technique that will make all your complex prompts easier to write and more effective. In the chapters to come, we&#x2019;ll build on this foundation. Each upcoming chapter of this book will delve into refining specific aspects of prompt writing. The very next pattern will focus on arguably the most important chapter of any prompt: clearly defining the task you want the LLM to perform. With a solid understanding of the Chapter pattern, you&#x2019;re already on your way to writing prompts that are easier to manage and that yield better results.</p>

                                
                                </section>
                            
    </div>
    <div class="search-results">
        <div class="has-results">
            
            <h1 class="search-results-title"><span class='search-results-count'></span> results matching "<span class='search-query'></span>"</h1>
            <ul class="search-results-list"></ul>
            
        </div>
        <div class="no-results">
            
            <h1 class="search-results-title">No results matching "<span class='search-query'></span>"</h1>
            
        </div>
    </div>
</div>

                        </div>
                    </div>
                
            </div>

            
                
                <a href="prompt_file.html" class="navigation navigation-prev " aria-label="Previous page: Prompt File">
                    <i class="fa fa-angle-left"></i>
                </a>
                
                
                <a href="problem_orientation.html" class="navigation navigation-next " aria-label="Next page: Problem Orientation">
                    <i class="fa fa-angle-right"></i>
                </a>
                
            
        
    </div>

    <script>
        var gitbook = gitbook || [];
        gitbook.push(function() {
            gitbook.page.hasChanged({"page":{"title":"Chapter","level":"2.3","depth":1,"next":{"title":"Problem Orientation","level":"3.1","depth":1,"path":"catalog/problem_orientation.md","ref":"catalog/problem_orientation.md","articles":[]},"previous":{"title":"Prompt File","level":"2.2","depth":1,"path":"catalog/prompt_file.md","ref":"catalog/prompt_file.md","articles":[]},"dir":"ltr"},"config":{"gitbook":"*","theme":"default","variables":{},"plugins":[],"pluginsConfig":{"highlight":{},"search":{},"lunr":{"maxIndexSize":1000000,"ignoreSpecialCharacters":false},"sharing":{"facebook":true,"twitter":true,"google":false,"weibo":false,"instapaper":false,"vk":false,"all":["facebook","google","twitter","weibo","instapaper"]},"fontsettings":{"theme":"white","family":"sans","size":2},"theme-default":{"styles":{"pdf":"styles/pdf.css","epub":"styles/epub.css","mobi":"styles/mobi.css","ebook":"styles/ebook.css","print":"styles/print.css","website":"styles/website.css"},"showLevel":false}},"structure":{"langs":"LANGS.md","readme":"README.md","glossary":"GLOSSARY.md","summary":"SUMMARY.md"},"pdf":{"pageNumbers":true,"fontSize":12,"fontFamily":"Arial","paperSize":"a4","chapterMark":"pagebreak","pageBreaksBefore":"/","margin":{"right":62,"left":62,"top":56,"bottom":56}},"styles":{"pdf":"styles/pdf.css","epub":"styles/epub.css","mobi":"styles/mobi.css","ebook":"styles/ebook.css","print":"styles/print.css","website":"styles/website.css"}},"file":{"path":"catalog/chapter.md","mtime":"2025-04-19T16:13:13.656Z","type":"markdown"},"gitbook":{"version":"3.2.3","time":"2025-04-19T16:13:44.653Z"},"basePath":"..","book":{"language":""}});
        });
    </script>
</div>

        
    <script src="../gitbook/gitbook.js"></script>
    <script src="../gitbook/theme.js"></script>
    
        
        <script src="../gitbook/gitbook-plugin-search/search-engine.js"></script>
        
    
        
        <script src="../gitbook/gitbook-plugin-search/search.js"></script>
        
    
        
        <script src="../gitbook/gitbook-plugin-lunr/lunr.min.js"></script>
        
    
        
        <script src="../gitbook/gitbook-plugin-lunr/search-lunr.js"></script>
        
    
        
        <script src="../gitbook/gitbook-plugin-sharing/buttons.js"></script>
        
    
        
        <script src="../gitbook/gitbook-plugin-fontsettings/fontsettings.js"></script>
        
    

    </body>
</html>

