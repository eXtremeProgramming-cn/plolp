# Example

*Provide concrete examples for complex steps to avoid confusion.*

## Motivation

For complex or abstract tasks, just giving instructions might not be enough for the LLM to understand exactly what's expected. If no examples are provided, the model could misinterpret the format or level of detail required. This often leads to confusion and outputs that don’t meet your needs—for instance, the model might use a completely different format or tone than the one you intended.

## Solution

Include concrete examples within your prompt to illustrate what you want. For example, show a sample input and the desired output for a key step of the task. By demonstrating the correct format or outcome with an example, you remove ambiguity and help the LLM follow your intentions more closely.

## Challenge

**The gap between instructions and understanding:** One of the biggest challenges in working with language models is that they have to infer what you want from your words alone. Humans rely on shared context and often **need examples** to clarify expectations—LLMs are no different. When a task is abstract or complex, a plain instruction can be interpreted in many ways by the model. Without an example to anchor the meaning, the LLM might confidently generate something that misses the mark completely. This pattern addresses the frustration many users face when an LLM output is technically “on topic” but not in the form or style they envisioned. It’s like asking a friend to "dress formally" for an event without telling them the event is a beach wedding—they might show up in a three-piece suit when you expected a nice shirt and khakis. The instruction wasn’t wrong, but it was too general. In the same way, saying *“write a brief”* or *“summarize this report professionally”* to an LLM might yield an unexpected result if you don’t also show what you mean by *“brief”* or *“professional.”*

**Abstract instructions can be ambiguous:** Many prompts use words like *concise*, *formal*, *friendly*, or *analysis*. These terms are open to interpretation. An educator might ask an LLM, *“Explain photosynthesis in simple terms.”* But *“simple”* for a PhD biologist could still be too complex for a 5th-grade science class. The result? The model’s explanation might include jargon or assume background knowledge that a young student doesn’t have. The teacher is left with an answer that’s technically correct but ineffective for the audience. Why did this happen? Because what the teacher envisioned as a *“simple explanation”* wasn’t explicitly demonstrated. If the prompt had included a short example of the style and level of detail—perhaps showing how a tough concept like gravity was explained in one sentence to 10-year-olds—the model would have a concrete target to emulate. Without that anchor, the model fell back on its general training, which might skew more academic than intended.

**Different format expectations:** Consider a government policy analyst (let’s call her Li) who is using an LLM to draft an **internal reference memo** for senior officials. Li knows these internal documents have a very specific style: direct language, a structured layout with sections, and a tone that is candid yet professional. She instructs the LLM, *“Write an internal reference report about the recent trade agreement, in formal Chinese, with analysis and recommendations.”* The model produces a report, but the format is wrong—it reads more like a news article or a public press release than an *internal reference (内参)* document. The content is there, but the structure and tone are off: it might lack the numbered sections and concise bullet points that leaders expect in an internal memo. What went wrong? Li described the desired style in words (“internal reference report, formal, with analysis and recommendations”), but those words were **too abstract** for the model to fully grasp. An LLM has learned from countless examples of text, but unless it has seen something exactly like an “internal reference document” in its training (which is unlikely, as these are confidential by nature), it will guess based on closest alternatives it knows. Without a concrete example, the LLM did its best, but its best guess missed the nuanced format Li needed. The challenge for users like Li is figuring out how to convey those nuances in a prompt.

**Relatable scenario – the journalist’s dilemma:** Now think of a journalist who wants to use an LLM to produce a short news digest. Alex, the journalist, asks the model: *“Summarize the following 3 news articles into one combined news brief.”* Alex expects a crisp, three-paragraph summary with a clear headline and source attributions, similar to the briefs published in his newsroom. The LLM does summarize the content, but the output is a single long paragraph that mushes everything together. There’s no headline, the tone is more akin to a casual blog post, and it fails to clearly separate the key points of each story. Alex is disappointed—this isn’t a usable news brief. The problem here is not that the model failed to follow instructions; it did condense the articles. But **without an example**, the model had to invent a structure for the brief, and it didn’t choose the one Alex had in mind. To the LLM, “news brief” could have meant many things. Perhaps it drew on an average of what it knows about summaries, or it gave a generic result. The challenge for professionals like journalists is that *they know exactly what a good news brief looks like*, but the model doesn’t share that context unless it’s spelled out or shown with a sample.

**Why we miscommunicate with LLMs:** Unlike humans, LLMs don’t truly *know* or *feel* the context—they rely on patterns. When you say *“write professionally,”* the model has to guess what mix of vocabulary, tone, and structure you consider professional. Different organizations or domains have different standards. For instance, a “professional” medical report looks very different from a “professional” legal brief or a “professional” press release. If you don’t give an example, the LLM might lean on a default style that doesn’t fit your needs. This is especially problematic for non-technical users like civil servants, teachers, or journalists who may assume the model “knows what I mean” because a human colleague likely would. We unconsciously expect the AI to have common sense about genre and format. But the AI’s common sense is just a statistical amalgamation of its training data. It has seen many formats and it might choose the wrong one in your case.

**Consequences of missing examples:** The lack of examples can lead to **poor or confusing outputs** that require extensive editing or, worse, can mislead the user. In a classroom setting, a teacher who asked for an *outline* of a chapter might receive a flowing essay instead. In a newsroom, an editor asking for a *fact box* could get a narrative paragraph, missing the point entirely. In government, a policy brief generated without seeing a prior brief might include information in the wrong order, burying critical recommendations at the end instead of highlighting them at the top. These outcomes all stem from the same root issue: the user’s mental image of the expected output wasn’t successfully conveyed to the LLM through instructions alone.

**The challenge of describing a style:** Sometimes, you *know* the output needs to have a certain style or voice, but describing that style in words is incredibly hard. You might try a bunch of adjectives: *“make it formal but engaging, detailed but not verbose, confident yet not aggressive.”* These could mean different things to different people—and to an LLM. A civil servant drafting a diplomatic cable with an AI’s help might instruct, *“Use a neutral and respectful tone, in standard government format.”* The model might still get it wrong, simply because terms like “neutral tone” and “standard format” are fuzzy. The user is essentially trying to compress years of writing conventions into a sentence or two of instructions. That’s a tall order! It’s no surprise that the result can be off-target. Without an example of a *diplomatic cable* or a snippet of a correctly formatted memo, the model is guessing what *neutral and respectful* should sound like in that context.

**Lessons from real use-cases:** People have discovered through trial and error that adding examples to their prompts often fixes these issues. The pattern of providing examples has emerged from practical needs. Early users of LLMs found that if the model’s output format was wrong, simply saying “No, in bullet points!” wasn’t always enough—it might still format incorrectly or inconsistently. But if the user instead added a quick example of a bullet-point list in the prompt, suddenly the model got the idea and followed it. The challenge is that this solution isn’t always obvious to users upfront. Many users will struggle through confusing outputs, not realizing the model would do better if only they *showed it what they wanted*. This pattern aims to bridge that knowledge gap and save users from that frustration.

**In summary,** the core challenge addressed by the *Example* pattern is the ambiguity of language and intention in prompts. Non-technical professionals often face this when using LLMs in their domain-specific work. Whether it’s government documentation, educational materials, or journalism, the lack of concrete examples in a prompt can lead the model to produce something that technically fits the instruction words but misses the spirit or format required. By recognizing this challenge, we set the stage for the solution: giving the model something solid to emulate, rather than leaving everything to its imagination (which, as we’ve seen, can run wild in the wrong direction).

## Example

So how do we put the solution into practice? The key is to **show, not just tell.** In this section, we’ll explore how adding examples to your prompts can guide an LLM to the desired output. We’ll look at a few scenarios and compare prompts without examples to improved prompts that include them. Along the way, we’ll discuss why the “with example” prompts work better, and how you can craft examples (inline or as attachments) effectively.

**1. Simple inline examples for clarity.** Even a brief example can resolve ambiguity in instructions. Suppose you want the model to transform text in a specific way. A classic case is reformatting data. Imagine you have a list of names and you need them in LASTNAME, Firstname order. If you just instruct, *“Reformat the following names into LASTNAME, Firstname,”* the model might do it correctly, but it might also get confused or only partially comply (especially if the concept of “LASTNAME, Firstname” isn’t crystal clear from the instruction alone). A quick inline example can eliminate the doubt. You could prompt: **“Reformat the following names (e.g. change `Michael Jordan` to `JORDAN, Michael`).”** By including the parenthetical example, you’ve shown exactly what transformation you expect. The model doesn’t have to guess the pattern—it sees it. For a task this straightforward, one example is usually plenty. You’ll likely get back a list where every name is in the correct “SMITH, John” format, because the LLM has essentially been given a template to follow. In contrast, without the example, some models might return *“Michael Jordan -> JORDAN, Michael”* as if explaining the rule, or they might incorrectly order parts of more complex names. The example anchors the model’s understanding: it now knows the output should look like `LASTNAME, Firstname` because you showed it exactly that.

**Why it works:** Language models excel at pattern matching. When you provide a pattern explicitly (even with a single illustration), the model will mimic that pattern in its completion. In the name format case, the pattern is simple: “Two words: first last -> last, FIRST.” Once shown, the model will apply it to all names by analogy. You’ve essentially done a mini training on the fly—this is sometimes called a “few-shot” prompt (in this case a one-shot, since you gave one example). For a non-technical user, you don’t need to know the term *few-shot learning*; just remember that an example functions as a demonstration. It’s very much like teaching by example in real life. If you were teaching someone how to format a bibliography entry, you’d show them one correctly formatted entry. The LLM learns in a similar way within the single prompt.

**2. Guiding style and tone with examples.** Let’s return to the earlier scenario of the journalist, Alex, who needed a combined news brief from multiple articles. We saw that just asking for a summary in a “news brief” style wasn’t enough—the model didn’t produce the right format. How could Alex fix this using the Example pattern? The solution is to show the model what a proper news brief looks like. Alex can include a short **template or example** in the prompt. For instance, before asking the model to summarize the actual articles, he might provide a made-up brief on a different topic as a model answer. 

He could write something like this in the prompt:

> **Example News Brief (for reference):**  
> **Category:** World News  
> **Title:** Major Trade Agreement Reached in Asia-Pacific  
> **Brief:** In a landmark deal, fifteen Asia-Pacific nations signed a trade agreement that is expected to boost regional economic growth. The accord, championed by China, will lower tariffs across multiple industries and could reshape supply chains. Observers note that the United States is absent from the pact, marking a significant shift in global trade dynamics. *Sources: Reuters, 2025-03-01; Xinhua, 2025-03-02*

Following this, Alex would then say: *“Now combine the following three articles into a similar brief:”* and provide the content to summarize. By including the example, Alex has given the AI a clear picture of the target format: a category label, a title, and a brief paragraph that synthesizes key points, plus source attributions at the end. The difference this makes can be dramatic. The model, seeing the example, will likely produce an output with the same structure and style for the new input articles. Instead of a muddled paragraph, Alex now gets something like:

- A clear category (e.g., **World News** or whatever is appropriate).
- A concise title highlighting the main story.
- A well-structured paragraph touching on the key facts and significance, written in that neutral journalistic tone.
- Perhaps a source citation pattern if that was shown in the example.

In essence, the example brief acts as a **template**. The LLM isn’t copying the content (the content is about a different topic entirely), but it’s mimicking the format and stylistic cues. 

Crucially, Alex made sure to label the example clearly as an example (“for reference”) so the model knows that part is just a guide, not something to continue writing about. This clarity helps the LLM separate *what is the example* from *what is the actual task*.

**Relatable case – teaching by example:** This approach works just as well for our educator scenario. Suppose a teacher, instead of just saying *“Explain photosynthesis in simple terms,”* adds, *“For example, here’s how I explained gravity to my class: ‘Think of Earth like a big magnet…’”* and then continues, *“Now explain photosynthesis in a similar style.”* The teacher’s short example about gravity (in a child-friendly way) provides a pattern. The model will pick up the informal, analogy-driven style and apply it to photosynthesis: perhaps it will talk about leaves as “little kitchens” for the plant, or something equally accessible, mirroring the simplicity and clarity demonstrated by the gravity example. The difference is night and day: the answer about photosynthesis is now pitched at the right level, using comparisons or language a 10-year-old can grasp, instead of being a mini textbook entry. By seeing one instance of the desired explanation style, the model generalizes to the new topic with much better alignment to what the teacher wanted.

**3. Complex formats and “standalone” example attachments.** The more complex the format or style you need, the more helpful (and lengthy) your examples might have to be. Sometimes a quick inline example isn’t sufficient to convey an entire format. Let’s revisit Li, the policy analyst writing an internal reference memo. She has a tough challenge because an internal reference document has multiple sections, a very particular tone, and perhaps even unwritten conventions that are hard to summarize in a sentence. In such cases, providing a **longer example as an attachment** can be extremely effective. 

What does that look like in practice? Li could take a previous internal reference (one that’s not sensitive, or perhaps a fictional one she creates based on real ones) and include it as an attached text or in a clearly separated section of her prompt. She might say in her prompt: *“See the attached example of an internal reference document. This is a sample for style and structure. Your task is to write a new internal reference about the given topic, following a similar format and tone. **Use the attachment only as a style guide** – the content of your report should be about the new topic, not about the example’s topic.”* Then, she provides the example document text separately.

For instance, her attachment (or a separate section below the instructions) could start with something like:

*(Attachment: Example Internal Reference Document)*

- *Title: Reflections on Recent Trade Frictions with Country X*  
- *Introduction:* (An opening that sets context and states the main issue directly…)  
- *Analysis:* 1. **Situation Overview:** … 2. **Underlying Causes:** … 3. **Implications for China:** …  
- *Recommendations:* … (short, numbered recommendations)

*(End of Attachment)*

Even if the attachment is somewhat long (say a few hundred words or more), it gives the LLM a concrete model of what the output should look like. It’s much more illustrative than saying “follow a standard internal reference format,” which, as we saw, was too vague. By reading the example document, the LLM can infer things like: the report starts with a clear title and introduction, uses enumerated sections in Chinese with certain phrasing, keeps a neutral yet authoritative tone, and perhaps uses specific terminology appropriate for internal reports. All these features would be hard to convey by description alone. But with an example, Li is essentially saying “write something kinda like this, but about my new topic.”

After providing the example, it’s important that Li explicitly instructs the model on how to use it. In the prompt, she clarified that the example is *only a style guide* and that the model should not copy any facts from it. This prevents the model from mistakenly carrying over irrelevant details. LLMs sometimes need that extra nudge to understand the purpose of the example. Otherwise, a less clear prompt might confuse the model – it could start continuing the example’s content or mix up the example with the actual task. Li avoids this by delineating the example (perhaps with a header “Attachment” or a different section) and telling the model what to do with it.

**Why attachments help:** By placing a long example in an attachment or at the end of the prompt, you keep your main instructions separate and clean, while still giving the model a wealth of guidance. For very stylistic or format-heavy tasks, this method shines. It’s akin to giving the model a reference manual or template to imitate. Professionals often do this with new human hires: if a new analyst is asked to write a report, they might be given a past report to use as a model. The same technique works for AI. In fact, we saw exactly this strategy in a real prompt used for training an AI on writing Chinese internal references: the prompt provided two sample internal reference documents as attachments. The instructions explicitly said those attachments were examples of structure and tone, and that the AI should study them but not treat their content as input to discuss. The result was that the AI produced a memo much closer to the desired style—precise, succinct, and following the expected outline—on the first try.

**4. Good prompt vs. bad prompt – a comparison:** Let’s solidify this pattern with a direct before-and-after comparison. Imagine we want the model to produce a short analytical summary of a policy issue, with a certain structured format (perhaps like an executive summary with bullet points for findings and recommendations). Here’s a **first attempt without using examples**:

> **Prompt (Attempt 1):** “Summarize the following policy report in a professional executive summary format with key findings and recommendations.”

Now, this sounds clear enough, but when we run it, the output might come as a single block of text, or it might list some findings and recommendations in a way that doesn’t quite look like our internal executive summaries. Maybe it writes a few paragraphs of narrative. It did summarize, but it didn’t present the information as bullet points, and perhaps it missed separating findings vs. recommendations distinctly, or it added unnecessary introductory fluff.

Now see the **improved prompt with the Example pattern** applied:

> **Prompt (Attempt 2):** “Summarize the following policy report as an executive summary. Use the format: first a brief introduction paragraph, then a bullet list of key findings, and a bullet list of recommendations. **For example,** here’s a template:  
> *Introduction:* One or two sentences stating the policy issue and overall finding.  
> *Findings:* \- Finding 1 …  
> \- Finding 2 …  
> \- (etc.)  
> *Recommendations:* \- Recommendation 1 …  
> \- Recommendation 2 …  
>   
> Now, read the report and produce an executive summary following this format.”

Notice how in Attempt 2 we explicitly gave a mini-template as an example (the text after “for example”). We showed the structure: an introduction label, then bullet points under Findings and Recommendations. We didn’t even need to fill in actual content for those points—we just wrote placeholders “Finding 1 …”. This is enough to illustrate the shape of the output. The model will almost certainly produce an introduction followed by bullet points labeled “Findings” and “Recommendations,” because we demonstrated exactly that in the prompt. In Attempt 1, we only described what we wanted; in Attempt 2, we demonstrated it. The difference is tangible in the output: Attempt 2’s output will be formatted as an executive summary with the right sections. Essentially, we cut down the guesswork. The model doesn’t have to imagine what we mean by “executive summary with key findings” because we showed a sketch of it.

**Avoiding confusion when giving examples:** We should address a subtle point: sometimes, if examples are not clearly set off or explained, the LLM can get a bit confused. For instance, if you just paste a big example without context, the model might not be sure if that’s something it should continue or something it should imitate with new data. To prevent this, always **separate your example from the prompt query**. You can use labels like “Example:” or “Sample output:” to flag that what follows is illustrative. Some users put examples at the end of the prompt and start them with a clear marker (like `<<< Example Begin >>>` … `<<< Example End >>>`). Others literally call it an attachment or explicitly say “The following is an example. Do NOT continue this example; it is only a reference.” By doing this, you ensure the LLM treats the example for what it is. In our news brief case, Alex wrote “Example News Brief (for reference)”—this kind of wording helps the model compartmentalize the example as just a guide.

Also, be specific about **what aspects of the example the model should follow**. Is it the tone? The structure? The level of detail? Or all of these? If your example story happens to be about a trade agreement and your actual task is about a health policy, you don’t want the model to fixate on trade jargon. Remind it: “Follow the style and format of the example, but write about the new topic.” This way, there’s no ambiguity about what to take from the example and what to ignore. LLMs don’t inherently know why you gave an example—you must tell them *why* and *how* to use it.

**Balancing detail in examples:** Another practical consideration is how big or detailed an example should be. If it’s a simple format or a small task, one or two line example (inline) is perfect and keeps the prompt efficient. For more complex outputs, a longer example might be needed, but remember that very large examples eat into the model’s context limit (and may incur higher costs if you’re using an API). So, use the smallest example that still clearly demonstrates the pattern. Sometimes a partial example is enough. You might not need a full report as an example—perhaps just the outline of one, as we did in the executive summary template. Or instead of attaching a 5-page internal report, maybe giving one section of it as an example of tone and structure could suffice. The goal is to illustrate the desired output as concretely as possible, **without overshadowing the actual task content**. The model should see the forest (the pattern) from the example, not get lost in the trees (the specific details of the example).

**Attachment vs inline – which to use?** Use inline examples (right in the prompt text) for quick, straightforward demonstrations or when the example is short. They work well for formatting rules, short answer styles, or tiny templates. Use attachments or clearly separated longer examples when the output format is lengthy or you need to provide a rich example without interrupting the flow of your instructions. In an attachment, you can include a lot more text (even multiple examples, if needed). Just be sure to reference them in your main prompt, like, “See Attachment 1 for an example of the report format.” Many advanced prompters actually prepare prompts in multiple parts: the main instructions in one file or section, and examples in another, to keep things organized. This also helps when others need to read or reuse your prompt – it’s structured and clear what each part is for.

**The payoff:** When you incorporate the Example pattern into your prompts, you’ll find that the LLM’s outputs require far less tweaking. The model’s interpretation of your request will more likely match what you envisioned on the first try. You’ll spend less time correcting the model’s format or asking it to rewrite in a different style. Instead, you front-loaded that effort by crafting a good example. It’s a bit more work for you at the prompt-writing stage, but it pays dividends in the quality of results. 

To sum up, providing examples in prompts is a powerful way to align the model’s output with your expectations. Whether it’s a single-sentence example to clarify a formatting rule or a full-paragraph example to set the tone, these concrete illustrations act as a guide rail for the LLM. As the old saying goes, “a picture is worth a thousand words” — in prompting, **an example can be worth a thousand instructions**. By showing the model what you need, you dramatically reduce confusion and get closer to the result you want, with far less back-and-forth. This pattern is all about that pragmatic, show-and-tell approach: when in doubt, give an example, and watch the LLM follow your lead.
