# Glossary  
*Provide a glossary with terminologies and definitions to avoid ambiguity.*

## Motivation  
If a prompt introduces specialized terms or acronyms without definitions, it can be ambiguous to both the LLM and human readers. The LLM might misinterpret a key term or use it inconsistently, while collaborators or readers could have differing interpretations of the same term. Such ambiguity undermines clarity and can lead to responses that are off-target or confusing.

## Solution  
Provide a glossary of key terms and their definitions as part of your prompt, covering each important or potentially ambiguous term with a concise explanation. You can integrate these definitions into a dedicated "Glossary" chapter in your prompt file. By defining terminology upfront, you ensure the LLM interprets each term as intended and uses those terms consistently. This practice also ensures any human collaborators or readers share the same understanding of these terms, eliminating confusion.

## Challenge  
Language is rich in nuance, and the same word can mean very different things to different people. An LLM user faces a fundamental challenge: ensuring that the AI **and** any human team members interpret critical terms in the prompt the same way. When key terminology is left undefined, ambiguity creeps in. The result? The LLM might take the prompt in an unintended direction, and team members might talk past each other without realizing it. In a task of any complexity, **undefined or ambiguous terms are a recipe for misunderstanding**. This section explores why that happens and the real-world problems it causes, highlighting why a glossary becomes necessary.

One common source of ambiguity is **inconsistent use of everyday words**. For example, consider the word **“article.”** In everyday language, *article* might refer to a news piece on a website or the physical item you’re reading. Now imagine a team working on a weekly news digest using an LLM. One member refers to the original news pieces as “articles,” while another calls the summaries that go into the newsletter “articles” as well. At first, the team might not notice the clash – after all, humans often rely on context and can clarify meaning on the fly. But an LLM doesn’t have the benefit of implicit understanding or real-time clarification. If the prompt says, *“Summarize each article and then list the key points from each article,”* what does *“article”* refer to in each instance? Without clarity, the model could misinterpret the instruction. It might, for instance, try to summarize the *summary itself* or double-summarize the original piece, leading to confusion in the output. The team could end up with an off-target result simply because the term “article” was used ambiguously. This kind of subtle miscommunication illustrates how an undefined term can derail the intended use of the AI.

Ambiguity doesn’t just confuse the AI – it also affects **collaboration among people**. In the scenario above, suppose the LLM returns a less-than-satisfactory output. One team member might blame the model’s abilities, not realizing the prompt itself was the culprit. Another member, interpreting “article” differently, might think the instructions were followed correctly. The team could spend hours tweaking the prompt or arguing over results, not immediately recognizing that a single word caused the divergence in understanding. In fast-paced projects or high-stakes environments, such misalignment can be costly. **When collaborators have different interpretations of a term, their efforts to guide or correct the LLM can work at cross purposes.** The prompt becomes a moving target—each person might rewrite it according to their own understanding, causing further inconsistency and frustration.

Another challenge arises with **acronyms and domain-specific jargon**. In many fields, people love to shorten complex phrases into neat acronyms (think NGO, GDPR, API) or use insider shorthand. If you include an acronym in a prompt without explanation, you are gambling that the model knows the term and, more importantly, knows which meaning you intend. For instance, the acronym “NOC” could mean **Network Operations Center** in an IT context, but in a news-writing project it might stand for **News On China**. If the prompt instructs: *“Using the NOC guidelines, select top stories,”* the LLM could be utterly confused if it hasn’t seen “NOC” before or if it associates it with the wrong concept. Even if the model has some idea (perhaps it was trained on texts where "NOC" appears), it might default to the wrong expansion of the acronym. The output in that case could veer into irrelevant territory. Similarly, domain jargon can trip up an AI. A medical team might prompt an LLM to *“summarize the patient’s ECG findings and note any indications of AF.”* Here “AF” stands for atrial fibrillation to the doctors, but the model might misinterpret it as “AF” the abbreviation for a different medical term or not expand it at all. The result could be a summary that misses the mark, either by avoiding the term (due to uncertainty) or by producing a generic explanation that doesn’t fit what the user needs.

To appreciate how ambiguity affects LLM output, it’s important to remember that **an AI lacks common sense and context beyond what you explicitly provide**. Unlike a human colleague, it cannot ask a clarifying question in the middle of generating an answer (unless explicitly prompted to do so). It takes your prompt at face value. If a term is ambiguous, the model will do *its best guess* based on its training data. That guess might not align with your intent. For example, if you ask the model to *“draft an article on climate,”* do you mean a news article about climate change? Or perhaps an article of a law (since “climate” could metaphorically refer to a political climate)? A human might ask, “Could you clarify what kind of article you mean by climate?” The model, however, will pick a path—say, writing about climate change—and if that’s not what you wanted, you’ll have to prompt again. This trial-and-error wastes time and can be frustrating. **Ambiguity forces the LLM to play a guessing game**, and there’s no guarantee it will guess the way you intended.

Now consider how easily glossary omissions happen in real-world scenarios. Often, project teams dive into using an LLM with enthusiasm, focusing on the big picture task (like “summarize the news” or “analyze these reports”) and assume everyone (including the AI) knows what they mean by the terms involved. In the rush, defining terms can seem pedantic or unnecessary—until something goes wrong. **Glossaries are frequently overlooked** due to a false sense of mutual understanding. Team members think certain words are self-explanatory. It’s only when the AI returns a perplexing answer or a teammate raises an eyebrow that they realize their “common” understanding wasn’t so common. For example, a team creating a financial report summary prompt might assume the term “ROI” (Return on Investment) is universally understood. But if the model generates a report and somewhere interprets ROI as “Region of Interest” (a term used in photography or medicine), parts of the output could be bizarre. Skipping the glossary step means such misunderstandings aren’t caught upfront.

What kind of terms are most prone to cause these issues? Here are a few categories of **potentially ambiguous terminology** that often challenge LLM users and their teams:

- **Acronyms and Abbreviations:** As mentioned, acronyms like *ROI*, *API*, *NOC*, or *SWOT* can have multiple meanings. If not spelled out, the model might guess wrong or not understand them at all. Always suspect that an acronym might not be as universally understood as you think.  
- **Overloaded Common Words:** Words that have several meanings depending on context, such as *“article,” “lead,” “note,” “client,”* or *“report.”* For instance, *“lead”* could mean a lead in sales (potential customer) or lead metal, or leading a project. In a prompt context, *“Prepare a report”* could confuse a model if earlier you referred to an input data file as “the report.”  
- **Domain Jargon:** Specialized terms used within a field or organization. In education, “IEP” stands for Individualized Education Program – critical for a summary of a student’s progress, but meaningless to an AI unless defined. In law, “brief” could be a summary of a case or a type of document; without clarity, an AI might not format or approach it correctly.  
- **Project-Specific or Team-Invented Terms:** Teams often develop shorthand or nicknames for things. Maybe your team refers to the new marketing plan as “Project Sunrise.” If you prompt the LLM with “Analyze how Project Sunrise performed,” the model won’t know what *Project Sunrise* is unless you’ve described it somewhere. Internal code names, product names, or even fictional characters in a storytelling prompt all need a line of explanation.

When glossaries are neglected, **the consequences become apparent in the LLM’s output and the team’s workflow**. The AI’s responses might include inconsistent usage of a term – for example, using an acronym in one paragraph and then suddenly spelling it out incorrectly in another, because it wasn’t sure. The tone or style might shift if the AI mistakenly treats two concepts as different when they were meant to be the same. In the worst case, the model’s answer could be entirely off-topic. Imagine expecting a summary of a client’s requirements for a “PC” (meaning *project charter* in your internal lingo) and the AI provides a summary of “personal computer” trends – a plausible interpretation from its perspective, but not what you needed. This kind of result can erode trust in the AI’s capabilities. The user might think the model is “not smart enough,” when in reality the instruction was too vague.

From the human side, overlooking a glossary can lead to **inefficient collaboration and repeated revisions**. One person might rewrite the prompt to clarify a term after seeing a confusing output, then another person, who had a different interpretation, might undo that change or alter something else. Without an agreed-upon set of definitions, each revision risks introducing a new form of the same ambiguity. This back-and-forth wastes valuable time and can be demoralizing. It’s much like a team of architects each using a different scale on their blueprints – the pieces won’t fit when assembled. In prompt design, if each team member mentally “scales” terms differently, the final prompt won’t function as intended for the AI.

In summary, the challenge that necessitates the Glossary pattern is **the omnipresent risk of ambiguity** in language. Small differences in understanding can lead to large differences in outcomes. An LLM cannot read your mind or infer context that isn’t given; it only has the prompt to work with. Likewise, fellow prompt designers or stakeholders may not share all your assumptions about terminology. The more complex the task, the more important it is to nail down the meaning of each key term from the start. By recognizing this challenge—that ambiguity is a silent saboteur of effective prompts—you can see why providing a glossary isn’t just pedantry, but a practical solution to ensure everyone and everything (humans and AI alike) is on the same page.

## Example  
Let’s walk through a concrete example to see how the Glossary pattern is applied in practice. Imagine you are part of a small team preparing a weekly **news summarization newsletter**. The goal is to use an LLM to generate concise summaries of various news articles, which will then be compiled into a newsletter for your readers. The task sounds straightforward: feed the news content to the model and get summaries. The team has drafted a multi-part prompt file to orchestrate this process. It includes sections like *Your Role* (setting the LLM as a journalist or editor), *Task Overview* (explaining the overall goal), and instructions for how to write the summary (tone, length, etc.). Initially, the prompt file did **not** include a glossary. We’ll see how that led to issues and how adding a glossary resolved them.

**Setting the Stage (Before Glossary):**  
In the first iteration, your team’s prompt file might have looked something like this (simplified for illustration):

- **Task:** “You are an assistant helping to create a news digest. Summarize each article provided and then write a brief introduction to the newsletter. Make sure each article’s summary is around two paragraphs long, capturing the key points of the article.”  
- **Content Provided:** (Here, the team would input several news articles as source material.)  
- **Instructions:** “For each article, provide a summary titled ‘Story’ followed by the content. After summarizing all articles, write an introduction for the newsletter that ties all the stories together.”

At first glance, this prompt seems okay. But notice the multiple uses of the word “article.” In the Task, *article* refers to each original news piece. Later, in “Make sure each article’s summary…,” the intent was to talk about each summary, but the wording still says *article’s summary*. And then you call the summary a **“Story”** in the output format. We have a tangle of terms: *article*, *summary*, *story*, and even *newsletter*. With no clear definitions, there’s a lot of room for the AI to guess and for the humans to be inconsistent. 

**The Problem Unfolds:**  
When the team ran this prompt, the output from the LLM was not as smooth as expected. Among the issues:
- The AI occasionally used the word “article” when it meant “story” and vice versa. In one summary, it began, “**Article 1:** This article discusses…,” and in another it used “**Story 2:** …” because the instructions weren’t crystal clear on what to call them. The labeling was inconsistent.
- The introduction to the newsletter was muddled. The model seemed unsure what to refer to the pieces as. It said, “In this newsletter, we have several articles that we summarized in the following stories.” That sentence itself is a bit confusing – it was parroting the confusion present in the prompt.
- One of the summaries was oddly double-length. Why? It seems the model might have interpreted “article’s summary is around two paragraphs” to mean it should *include two paragraphs from the article* (perhaps quoting) and then add a summary, rather than write a two-paragraph summary. The phrasing wasn’t clear to the AI, and it made a wrong guess.

After seeing these issues, the team realized the **lack of clarity in terminology** was a major contributing factor. They had essentially given the model a puzzle: figure out what we mean by article vs. story. The model’s inconsistent output was a direct reflection of the inconsistent terms in the prompt. It was time to apply the Glossary pattern to fix this.

**Implementing the Glossary Pattern:**  
The team decided to add a dedicated **Glossary section** to their prompt file. Here’s how they went about it step by step:

1. **Identify Ambiguous and Key Terms:** The team reviewed the prompt and highlighted terms that could be interpreted in more than one way or were crucial to the task. They quickly picked out *“article,” “story,” “summary,”* and *“newsletter”* as the main ones. They also noted *“source”* might need clarification (is it the news source or the source as in source vs summary?). Essentially, any term that they found themselves using loosely or interchangeably went on the list. A quick test they used was to ask, “Could a new team member or the AI misunderstand what this refers to?” If yes, it needs defining.

2. **Define Each Term Clearly:** For each term, they wrote a concise definition, aiming for a single sentence if possible. The definitions focused on the context of their project. The glossary they drafted looked like this:

   **Glossary:**  
   - **Source article** – an original news article from an external publisher that we want to summarize. (For example, a news story from *The South China Morning Post* website.)  
   - **Newsletter story** – a written summary of a source article, prepared for inclusion in our newsletter. This is essentially the output the LLM will generate for each source article.  
   - **Newsletter** – the final compiled email or document that contains all the newsletter stories (summaries), plus an introduction and any other sections (like a conclusion or highlights).  
   - **Introduction** – a brief opening section of the newsletter that outlines what’s to come, often giving context or linking the stories together.  
   *(If there were any other terms specific to their project, those would be listed too. In this scenario, these four were the key ones.)*

3. **Integrate the Glossary into the Prompt File:** They placed this glossary near the beginning of the prompt file, right after the Task Overview section and before diving into the detailed instructions. The idea was to give the LLM the context of these definitions upfront. In the prompt file, it looked something like:

   ```
   3. Glossary  
   - Source article: an original news article from an external publisher that we want summarized.  
   - Newsletter story: the summary of a source article, written for our newsletter.  
   - Newsletter: the compiled document containing all the stories (summaries) and additional sections like the introduction.  
   - Introduction: the opening section of the newsletter that introduces the stories.
   ```

   By explicitly labeling this as a glossary section, it was clear to anyone reading the prompt file, and to the LLM, that these are reference definitions, not part of the narrative or instructions to output. (In practice, an LLM will usually not regurgitate the glossary in its answer unless asked; it uses it to understand terms in the instructions.)

4. **Adjust the Prompt Wording to Use Glossary Terms Consistently:** With the glossary in place, the team also went through the rest of the prompt and aligned the terminology. For example, instead of saying “Summarize each article,” the prompt was revised to say “Summarize each **source article** into a **newsletter story**.” And where they had “each article’s summary,” they changed it to “each **newsletter story** should be around two paragraphs long.” The introduction instruction became, “After writing all the newsletter stories, provide an **Introduction** for the **newsletter** that ties all the stories together.” These edits ensure that the language in the prompt matches the definitions in the glossary exactly. Consistency is key – the glossary can’t save you if you still sprinkle in undefined terms elsewhere. The team was careful now to stick to *either* “source article” or “story,” and not casually say just “article” or “summary” without context.

With these steps, the prompt file now had a coherent set of terms defined and used deliberately. The difference this made was immediately noticeable.

**The Outcome (After Glossary):**  
Upon running the LLM with the updated prompt (now including the glossary), the output improved markedly:
- Each summary was correctly labeled and had the expected content. The model output something like:
  - **Story 1:** *[Summary of source article 1]* 
  - **Story 2:** *[Summary of source article 2]*, and so on.
  It no longer mixed up “article” vs “story” because the prompt explicitly told it what a “story” is (a summary) and what an “article” is (the input). The LLM followed suit, using the terminology consistently in its response.
- The introduction at the top of the newsletter was clear and on point. For example, the AI wrote: “**Introduction:** This week’s newsletter brings you insights from several important news stories, from climate breakthroughs to economic updates. Each **newsletter story** below distills a key article from the week into a concise summary for easy reading.” Notice how the introduction now even uses the term “newsletter story” correctly, showing the model understood and adopted the defined terms.
- There were no odd double-length summaries or misunderstandings of the instructions. By clarifying “each newsletter story should be around two paragraphs,” the model did exactly that – each summary was two paragraphs, giving a balanced level of detail. The ambiguity that led the model to potentially include quoted text or misunderstand the requirement was gone.

Beyond the improvement in the AI’s output, an interesting side benefit emerged: **team alignment**. With the glossary written down, everyone on the team had a reference. If a new colleague joined the project, they could read the prompt file and immediately grasp the jargon and shorthand being used. No one would be left wondering, *“Hang on, when you say story here, do you mean the original piece or the summary?”* because it was spelled out. In our scenario, when someone suggested a change like “Should we maybe call the source articles ‘originals’ to avoid confusion?”, the team could discuss it using the same terms and update the glossary and prompt in one go. Having a glossary turned out to streamline not just the LLM’s understanding, but the humans’ workflow as well. It became a single source of truth for what each term means in the context of the project.

**Applying the Glossary Pattern Generally:**  
The news digest example illustrates how a glossary can resolve ambiguity, but this pattern applies to **any project involving prompts**, especially lengthy or complex ones. The process to apply it is generally the same:
1. **Scan your prompt (or prompt plan) for terms that carry specific meaning or could be interpreted in multiple ways.** These might be technical terms, roles, acronyms, or even common words used in a special way. Trust your intuition and the “new person test” – if a new person would need an explanation for a term, put it in the glossary.
2. **Write clear, concise definitions for each of those terms.** Aim for one sentence, avoiding using other ambiguous terms in the definition. If a term is very complex, you might use two sentences or break it into sub-definitions, but brevity is usually helpful for quick reference.
3. **Place the glossary in your prompt structure where it will be seen and applied by the LLM.** Typically, this is at the beginning or near the top of your prompt file. You want the model to read these definitions before it encounters the terms in instructions. If you’re following a structured prompt file with chapters or sections (as recommended by other patterns in this book), the Glossary would be one of the early sections, right after the overall task description or context.
4. **Maintain and update the glossary as needed.** If your prompt evolves and new important terms appear, add them. If a term is no longer used, you can remove it to avoid clutter. Consistency is the goal, so your prompt and glossary should always reflect each other. If the glossary says one thing but the prompt text uses a different term, update one or the other so they align.

Remember that the glossary is there to serve you and the AI – it’s not an academic exercise in exhaustive definitions. **Keep it focused on what’s relevant to your prompt’s success.** For example, if you’re writing a prompt to draft a legal contract, it might help to define “Parties” (who are the entities involved) or “Effective Date” if those need to be clearly understood by the model. But you wouldn’t need to define “contract” or “law” – those are general concepts the model knows and everyone agrees on. The glossary should target the terms that are *important and potentially confusing in your specific context*.

**Real-World Reflection:** Many experienced prompt designers and AI users eventually develop an instinct for when to include a glossary. If you find that you’re re-explaining what something means in multiple prompts or seeing recurrent confusion, that’s a strong sign a glossary would help. In team settings, having a glossary can be the difference between a smooth collaboration and a series of miscommunications. It’s akin to establishing a brief **“dictionary of our project’s language”** before getting into the heavy lifting. Just as a business contract often begins by defining key terms (“‘Company’ shall mean X Corporation headquartered at…”) or a scientific paper has a section clarifying terminology, a prompt benefits from the same clarity.
