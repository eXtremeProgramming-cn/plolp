
<!DOCTYPE HTML>
<html lang="" >
    <head>
        <meta charset="UTF-8">
        <meta content="text/html; charset=utf-8" http-equiv="Content-Type">
        <title>Long Prompt Â· GitBook</title>
        <meta http-equiv="X-UA-Compatible" content="IE=edge" />
        <meta name="description" content="">
        <meta name="generator" content="GitBook 3.2.3">
        
        
        
    
    <link rel="stylesheet" href="../gitbook/style.css">

    
            
                
                <link rel="stylesheet" href="../gitbook/gitbook-plugin-highlight/website.css">
                
            
                
                <link rel="stylesheet" href="../gitbook/gitbook-plugin-search/search.css">
                
            
                
                <link rel="stylesheet" href="../gitbook/gitbook-plugin-fontsettings/website.css">
                
            
        

    

    
        
    
        
    
        
    
        
    
        
    
        
    

        
    
    
    <meta name="HandheldFriendly" content="true"/>
    <meta name="viewport" content="width=device-width, initial-scale=1, user-scalable=no">
    <meta name="apple-mobile-web-app-capable" content="yes">
    <meta name="apple-mobile-web-app-status-bar-style" content="black">
    <link rel="apple-touch-icon-precomposed" sizes="152x152" href="../gitbook/images/apple-touch-icon-precomposed-152.png">
    <link rel="shortcut icon" href="../gitbook/images/favicon.ico" type="image/x-icon">

    
    <link rel="next" href="prompt_file.html" />
    
    
    <link rel="prev" href="../" />
    

    </head>
    <body>
        
<div class="book">
    <div class="book-summary">
        
            
<div id="book-search-input" role="search">
    <input type="text" placeholder="Type to search" />
</div>

            
                <nav role="navigation">
                


<ul class="summary">
    
    

    

    
        
        <li class="header">Opening</li>
        
        
    
        <li class="chapter " data-level="1.1" data-path="../">
            
                <a href="../">
            
                    
                    Introduction
            
                </a>
            

            
        </li>
    

    
        
        <li class="header">Organizing Prompts</li>
        
        
    
        <li class="chapter active" data-level="2.1" data-path="long_prompt.html">
            
                <a href="long_prompt.html">
            
                    
                    Long Prompt
            
                </a>
            

            
        </li>
    
        <li class="chapter " data-level="2.2" data-path="prompt_file.html">
            
                <a href="prompt_file.html">
            
                    
                    Prompt File
            
                </a>
            

            
        </li>
    
        <li class="chapter " data-level="2.3" data-path="chapter.html">
            
                <a href="chapter.html">
            
                    
                    Chapter
            
                </a>
            

            
        </li>
    

    
        
        <li class="header">Defining Tasks</li>
        
        
    
        <li class="chapter " data-level="3.1" data-path="problem_orientation.html">
            
                <a href="problem_orientation.html">
            
                    
                    Problem Orientation
            
                </a>
            

            
        </li>
    
        <li class="chapter " data-level="3.2" data-path="prescribed_process.html">
            
                <a href="prescribed_process.html">
            
                    
                    Prescribed Process
            
                </a>
            

            
        </li>
    
        <li class="chapter " data-level="3.3" data-path="example.html">
            
                <a href="example.html">
            
                    
                    Example
            
                </a>
            

            
        </li>
    

    
        
        <li class="header">Setting Context</li>
        
        
    
        <li class="chapter " data-level="4.1" data-path="persona.html">
            
                <a href="persona.html">
            
                    
                    Persona Adoption
            
                </a>
            

            
        </li>
    
        <li class="chapter " data-level="4.2" data-path="audience.html">
            
                <a href="audience.html">
            
                    
                    Audience
            
                </a>
            

            
        </li>
    
        <li class="chapter " data-level="4.3" data-path="epistemology_frame.html">
            
                <a href="epistemology_frame.html">
            
                    
                    Epistemology Frame
            
                </a>
            

            
        </li>
    
        <li class="chapter " data-level="4.4" data-path="glossary.html">
            
                <a href="glossary.html">
            
                    
                    Glossary
            
                </a>
            

            
        </li>
    

    
        
        <li class="header">Structuring Input</li>
        
        
    
        <li class="chapter " data-level="5.1" data-path="exhaustive_input.html">
            
                <a href="exhaustive_input.html">
            
                    
                    Exhaustive Input
            
                </a>
            

            
        </li>
    
        <li class="chapter " data-level="5.2" data-path="online_search.html">
            
                <a href="online_search.html">
            
                    
                    Online Search
            
                </a>
            

            
        </li>
    
        <li class="chapter " data-level="5.3" data-path="attachment.html">
            
                <a href="attachment.html">
            
                    
                    Attachment
            
                </a>
            

            
        </li>
    
        <li class="chapter " data-level="5.4" data-path="relevance.html">
            
                <a href="relevance.html">
            
                    
                    Relevance
            
                </a>
            

            
        </li>
    

    
        
        <li class="header">Shaping Output</li>
        
        
    
        <li class="chapter " data-level="6.1" data-path="structured_output.html">
            
                <a href="structured_output.html">
            
                    
                    Structured Output
            
                </a>
            

            
        </li>
    
        <li class="chapter " data-level="6.2" data-path="style_mimicry.html">
            
                <a href="style_mimicry.html">
            
                    
                    Style Mimicry
            
                </a>
            

            
        </li>
    
        <li class="chapter " data-level="6.3" data-path="deconstruction.html">
            
                <a href="deconstruction.html">
            
                    
                    Deconstruction
            
                </a>
            

            
        </li>
    

    
        
        <li class="header">Sharpening Your Mind</li>
        
        
    
        <li class="chapter " data-level="7.1" data-path="never_stop_winning.html">
            
                <a href="never_stop_winning.html">
            
                    
                    Never Stop Winning
            
                </a>
            

            
        </li>
    
        <li class="chapter " data-level="7.2" data-path="iteration.html">
            
                <a href="iteration.html">
            
                    
                    Iteration
            
                </a>
            

            
        </li>
    
        <li class="chapter " data-level="7.3" data-path="decomposition.html">
            
                <a href="decomposition.html">
            
                    
                    Decomposition
            
                </a>
            

            
        </li>
    
        <li class="chapter " data-level="7.4" data-path="precision.html">
            
                <a href="precision.html">
            
                    
                    Precision
            
                </a>
            

            
        </li>
    
        <li class="chapter " data-level="7.5" data-path="brainstorm.html">
            
                <a href="brainstorm.html">
            
                    
                    Brainstorm
            
                </a>
            

            
        </li>
    
        <li class="chapter " data-level="7.6" data-path="criticism.html">
            
                <a href="criticism.html">
            
                    
                    Criticism
            
                </a>
            

            
        </li>
    

    

    <li class="divider"></li>

    <li>
        <a href="https://www.gitbook.com" target="blank" class="gitbook-link">
            Published with GitBook
        </a>
    </li>
</ul>


                </nav>
            
        
    </div>

    <div class="book-body">
        
            <div class="body-inner">
                
                    

<div class="book-header" role="navigation">
    

    <!-- Title -->
    <h1>
        <i class="fa fa-circle-o-notch fa-spin"></i>
        <a href=".." >Long Prompt</a>
    </h1>
</div>




                    <div class="page-wrapper" tabindex="-1" role="main">
                        <div class="page-inner">
                            
<div id="book-search-results">
    <div class="search-noresults">
    
                                <section class="normal markdown-section">
                                
                                <h1 id="long-prompt">Long Prompt</h1>
<p><em>Are you really using your LLM, if your prompt is under 1000 words?</em></p>
<h2 id="motivation">Motivation</h2>
<p>Short or overly brief prompts often fail to give a large language model (LLM) enough context or detail. When a query isn&apos;t comprehensive, the model might misunderstand the task or overlook important elements. As a result, the output is frequently shallow, generic, or missing critical information. Many users experience this: they pose a quick one-line question and get a mediocre, overly general answer in return. Essentially, <strong>garbage in, garbage out</strong> holds true: a flimsy prompt yields a flimsy answer.</p>
<h2 id="solution">Solution</h2>
<p>The solution is straightforward: make your prompt significantly longer and more detailed. Include extensive background information, context, and explicit instructions&#x2014;often pushing the prompt to 1000 words or more&#x2014;so the LLM fully understands what you need. With ample context provided upfront, the model can produce a much more thorough, accurate, and tailored response. In short, for complex tasks, don&apos;t hesitate to supply a multi-page prompt; the detailed guidance will pay off in the quality of the results.</p>
<h2 id="challenge">Challenge</h2>
<p><strong>The Problem with Short Prompts:</strong> Many people&#x2019;s first attempts with an LLM result in disappointing answers. The root cause is usually that the prompt (the question or instruction given) was too short. A one-sentence request rarely contains enough information to guide the AI toward a high-quality solution. In practice, an overly brief prompt can lead to several issues:</p>
<ul>
<li><strong>Superficial Answers:</strong> The AI may return a generic, surface-level response because it wasn&#x2019;t given specifics to elaborate on. For example, asking <em>&#x201C;Give me a summary of this report.&#x201D;</em> without context might yield a vague summary that states the obvious. </li>
<li><strong>Hallucinated Details:</strong> If important facts or context aren&#x2019;t provided, the model might fill in the gaps with plausible-sounding but incorrect details (a phenomenon often called &#x201C;hallucination&#x201D;). You could end up with output that includes facts or references that were never in your source material. </li>
<li><strong>Lack of Coherence or Focus:</strong> With minimal guidance, longer outputs can meander or become disorganized. The model might start strong but then wander off-topic or merge unrelated points because the prompt didn&#x2019;t set clear boundaries. </li>
<li><strong>Tone Mismatch:</strong> A short prompt usually doesn&#x2019;t specify the desired tone or style. If you expected a formal report but didn&#x2019;t say so, you might receive a casual or inconsistent tone. The default style the AI chooses could be inappropriate for your intended audience.</li>
</ul>
<p>All these issues stem from not giving the model enough to work with. Essentially, a short prompt leaves too many degrees of freedom (or too much uncertainty) in the task. The LLM has a vast amount of knowledge and ways it could respond, and with scant guidance it has to guess what you want. And unlike a human assistant, the AI won&#x2019;t usually ask clarifying questions if your instructions are vague&#x2014;it will simply forge ahead with whatever little you provided. The model might produce an answer that sounds confident but misses the mark, or include details it assumed (incorrectly) to fill in blanks. Sometimes it guesses wrong.</p>
<p><strong>An &#x201C;Aha&#x201D; Moment &#x2013; The Power of a Long Prompt:</strong> I vividly remember the first time I saw a truly <em>long</em> prompt in action. A colleague of mine, author Evgeny Morozov, shared a prompt he uses for analyzing entire books. It spanned more than ten pages of text, laying out in detail how the AI should approach summarizing and critiquing a book. Reading through this multi-thousand-word prompt was an eye-opener. Up until then, I had always felt that LLMs were falling short on complex tasks I tried &#x2014; the outputs weren&#x2019;t thorough, often felt superficial, sometimes even included made-up content or the wrong tone. After seeing Evgeny&#x2019;s approach, it suddenly clicked: the problem wasn&#x2019;t the AI&#x2019;s capability, it was my prompting approach. I realized that if I don&#x2019;t offer a long enough prompt to the LLM, I&#x2019;m not really giving it a fair chance to perform well. In other words, the AI can only think as thoroughly as the prompt allows.</p>
<p>This realization is backed up by a bit of simple logic. Think of the information in your prompt as constraints on the AI&#x2019;s response. A complex task usually has many moving parts &#x2014; if your prompt only covers a few of them, the model has to <em>improvise</em> the rest. In information science terms, a short prompt leaves a lot of entropy (uncertainty) for the model to resolve. There are countless plausible ways to answer, so the result may be unpredictable or not aligned with what you had in mind. By supplying more details and context, you dramatically narrow down the possibilities. You&#x2019;re essentially reducing the entropy by providing more &quot;bits&quot; of information for the model to work with, thereby guiding it toward the specific outcome you want. It&#x2019;s akin to giving a student a detailed assignment brief instead of a one-line instruction &#x2014; the detailed brief makes it much more likely the student (or in our case, the AI) will deliver exactly what&#x2019;s needed.</p>
<p>Empirical experience even suggests a rough rule of thumb: about 1000 words of prompt text is a normal threshold to clearly specify a non-trivial task. That may sound like a lot, but think about explaining a complex project to a colleague &#x2014; it wouldn&#x2019;t be surprising if it takes several pages of notes to cover all the nuances. The same goes for explaining the task to an AI. The point is not the exact word count, but that you shouldn&#x2019;t shy away from going into detail. If the task is important and complex, investing that effort in the prompt is worthwhile.</p>
<p><strong>Why One Long Prompt Beats Twenty Short Prompts:</strong> Some users try to compensate for a skimpy initial query by having a lengthy back-and-forth conversation with the AI. They&#x2019;ll ask a question, get a mediocre answer, then try to refine it with follow-up prompts, and so on. While iterative prompting can sometimes improve results, it has drawbacks compared to a well-crafted single prompt. A single, well-crafted long prompt has several advantages over trying to achieve the same result through many small prompts or multiple chat turns:</p>
<ul>
<li><strong>Complete Context at Once:</strong> In one long prompt, you can give <em>all</em> the instructions, information, and context up front. The model then sees the big picture and can plan its response with full knowledge of what&#x2019;s needed. In a multi-turn conversation, the AI only sees piecemeal information each round, which can limit its ability to integrate everything cohesively.</li>
<li><strong>Avoiding Drift:</strong> Long, multi-turn discussions can cause the model to drift off course or fixate on earlier misinterpretations. With each exchange, there&#x2019;s a risk of the AI taking the conversation down a sidetrack. Providing a comprehensive prompt in one go helps the AI stay focused on the main objective, rather than chasing new angles that come up in a back-and-forth chat.</li>
<li><strong>Consistency and Completeness:</strong> A carefully written long prompt can explicitly ensure that no aspect of the task is missed. You can instruct the model not to skip any step or section. (For instance, one book-summary prompt emphasizes <em>&#x201C;No chapter may be skipped or given abbreviated treatment&#x201D;</em>.) In a conversation, it&apos;s easier to inadvertently overlook some detail until you see it missing in the output. With a single prompt, you outline everything ahead of time.</li>
<li><strong>Reusable and Shareable Prompt &#x201C;Templates&#x201D;:</strong> A long prompt, once written, becomes a sort of blueprint for that task. You can reuse it whenever you need to perform the task again, or share it with colleagues who have similar needs. Because it&#x2019;s all in one self-contained text, it&#x2019;s easy to save and reuse. (The next pattern in this book, &quot;Prompt File&quot;, will discuss how to store and reuse prompts effectively.)</li>
<li><strong>Clarifying Your Own Requirements:</strong> Writing out a detailed prompt forces <em>you</em> to think through exactly what you want from the AI. It&#x2019;s like writing an outline or set of instructions for a human. Often, this process will reveal some of your own assumptions or areas you hadn&#x2019;t fully considered. By the time you finish writing the prompt, not only is the AI set up for success, but you yourself have a much clearer picture of the task at hand.</li>
</ul>
<p>None of this is to say that you should never use short prompts. There are certainly times when a quick, casual interaction with an LLM is fine&#x2014;especially for simple queries or early brainstorming of ideas (the &quot;Brainstorm&quot; pattern later in this book covers those scenarios). If you just need a few quick suggestions or you&apos;re exploring a topic interactively, you might start with shorter prompts. But when you have a well-defined, complex task that you want the AI to execute reliably, that&#x2019;s when the Long Prompt pattern shines. You gather all the relevant information and instructions and present them to the model in one comprehensive package.</p>
<p><strong>The Challenge of Writing a Long Prompt:</strong> At this point you might be thinking, &#x201C;This sounds great, but writing a 1000+ word prompt is a lot of work!&#x201D; It&#x2019;s true that crafting such a prompt requires effort. Many users aren&#x2019;t used to articulating their requests in such detail. It can feel like writing a mini-essay or a detailed specification document. The good news is that, like any skill, you get better at it with practice&#x2014;and the upcoming patterns in this book will break down the process so you can learn how to do it step by step. In the next sections, we&#x2019;ll explore techniques to structure and refine your prompts. For now, keep in mind that investing time in a long prompt can save you even more time that you would otherwise spend fixing or editing a poor AI-generated result. To see just how much of a difference a long prompt can make, let&apos;s walk through an example of this pattern in action.</p>
<h2 id="example">Example</h2>
<p>Imagine you are a policy analyst who needs to summarize a complex 300-page education reform report for a briefing. This is a substantial document with multiple sections, data, and recommendations. You want the LLM to produce a detailed summary that captures all the key points accurately and in a well-organized manner. We&#x2019;ll illustrate how using a Long Prompt yields a far better result for this task compared to a short prompt.</p>
<h3 id="attempt-1-a-short-prompt-what-goes-wrong">Attempt 1: A Short Prompt (What Goes Wrong)</h3>
<p>You start with a straightforward request to the AI: <em>&#x201C;Summarize the key points of the 300-page education reform report.&#x201D;</em> That prompt is only one sentence long. Let&#x2019;s say the AI produces an output like this:</p>
<p><strong>Output from short prompt:</strong>  </p>
<blockquote>
<p>The report covers various aspects of education reform. It suggests updates to the curriculum, improvements in teacher training, better educational infrastructure, and increased funding. Overall, it emphasizes the importance of modernizing the education system to improve outcomes.</p>
</blockquote>
<p>This response isn&#x2019;t <em>wrong</em>, but it&#x2019;s very generic. It sounds like a mediocre summary that could apply to almost any education report. Crucial details are missing: Which specific curriculum changes were proposed? What exactly should be improved in teacher training? Did the report provide data or case studies? Are there specific recommendations or just broad themes? The AI&#x2019;s answer glosses over all the nuance because we didn&#x2019;t give it much to go on. In essence, the model produced a bland overview because our prompt was so thin.</p>
<h3 id="attempt-2-a-long-prompt-getting-it-right">Attempt 2: A Long Prompt (Getting it Right)</h3>
<p>Now, let&apos;s apply the <strong>Long Prompt</strong> pattern to this same task. Instead of one sentence, we&#x2019;re going to provide the AI with a rich, detailed prompt. We&#x2019;ll include context, instructions, and expectations so that the model knows exactly what we want. Here&#x2019;s how we can build it, piece by piece:</p>
<ol>
<li><p><strong>Define the Role or Perspective:</strong> It often helps to tell the AI who it is or what perspective to take. In our case, we might say it is an expert education policy analyst. For example, the prompt could start: <em>&#x201C;You are an education policy analyst with expertise in national education reform.&#x201D;</em> This sets the tone and tells the model to respond in a knowledgeable, analytical manner. (Many successful long prompts begin by establishing a role; for instance, another prompt for a different task might say <em>&#x201C;You are a Chinese professor focusing on international relations&#x2026;&#x201D;</em> to give the model a persona and context.)</p>
</li>
<li><p><strong>Explain the Task in Detail:</strong> Don&#x2019;t just say &#x201C;summarize&#x201D; &#x2013; specify the scope and depth. For our report, we want a structured summary. We might write: <em>&#x201C;Your task: Provide a detailed, section-by-section summary of the report, including the introduction, each chapter&#x2019;s findings, and the conclusion. For each section, identify the key points and any important data or recommendations. The summary should be thorough but focused on the main insights that a policymaker needs to know.&#x201D;</em> This makes it clear that we expect coverage of every section, not just a high-level gloss. (In fact, a real book-analysis prompt explicitly stated the assistant <em>&#x201C;will produce a detailed, chapter-by-chapter ... summary of the entire book,&#x201D;</em> even specifying an expected output of <em>12,000&#x2013;15,000 words for a 70,000-word book</em>.)</p>
</li>
<li><p><strong>Provide Necessary Context or Background:</strong> If the AI needs any background information to do a good job, include it. In our example, the model might not know specifics of this report since it&apos;s a hypothetical internal document. So we can embed some key context into the prompt. For instance: <em>&#x201C;Background: The report, titled &#x2018;Education for the Future&#x2019;, was commissioned by the Ministry of Education in 2024 after a nationwide study. It covers four main areas: curriculum reform, teacher professional development, infrastructure investment, and educational outcomes (including comparisons with international benchmarks). For example, it includes case studies from pilot programs in several districts and statistical data on student performance.&#x201D;</em> By adding these details, we give the AI concrete material to anchor its summary. We&#x2019;re reducing the chance it will make something up or miss a major part. (In some cases, context might also mean providing excerpts or data from the document itself. If certain facts or figures are crucial, you can copy those into the prompt. Essentially, you&#x2019;re feeding the model the raw material it should draw from.)</p>
</li>
<li><p><strong>Outline the Structure of the Desired Output:</strong> To ensure the answer is well-organized, tell the AI how to structure it. We could say: <em>&#x201C;Format your summary in the following structure: Write a brief overview, then separate sections with headings for each major part of the report (Curriculum, Teacher Training, Infrastructure, Outcomes, etc.). Under each heading, bullet or enumerate the key findings and recommendations. Finally, provide a one-paragraph conclusion that highlights the overall message of the report.&#x201D;</em> This is very specific, but it gives the model a clear blueprint to follow. We won&#x2019;t get a rambling wall of text; we&#x2019;ll get a nicely formatted summary. (Similarly, prompts can specify structure in other contexts. For instance, a prompt for an internal briefing might instruct the AI to write an Introduction, followed by sections I, II, III such as <em>Situation, Analysis, Recommendations</em>, mirroring an established template.)</p>
</li>
<li><p><strong>Specify the Style and Tone:</strong> Now we tell the model how the summary should sound. For a policy briefing, perhaps we want a formal, concise tone. We might add: <em>&#x201C;Writing style: Use a formal, professional tone appropriate for senior officials. The summary should be concise and factual, without opinionated language or fluff. Assume the reader is knowledgeable about education policy, so use clear terms but no oversimplification. Avoid jargon where possible, and keep the language neutral and informative.&#x201D;</em> This guidance helps avoid the tone mismatch we saw earlier. The AI will model its language on these instructions. (For example, one long prompt in another domain specifies: <em>&#x201C;Tone: Neutral but subtly emphasize sovereignty and multilateralism, and avoid a tone of overt propaganda&#x201D;</em>. Such directions ensure the output aligns with the intended voice and perspective.)</p>
</li>
<li><p><strong>Add Constraints or Important Requirements:</strong> If there are any non-negotiable requirements, state them plainly. In our case: <em>&#x201C;Important: Do not omit any of the four main areas of the report in the summary. Each section (curriculum, teacher training, infrastructure, outcomes) must be covered with its key points. Also, do not fabricate any details&#x2014;base the summary only on the information provided in the report background.&#x201D;</em> This reminds the AI of the boundaries. We&#x2019;re effectively saying &#x201C;cover everything, and don&#x2019;t make stuff up.&#x201D; In other long prompts, authors often include similar strict requirements. Recall the earlier example where the prompt insisted on covering every chapter of a book with equal depth. That kind of instruction in the prompt enforces completeness and accuracy.</p>
</li>
</ol>
<p>After assembling all these elements, our prompt is quite lengthy &#x2013; and that&#x2019;s the goal. We might end up with a prompt well over a thousand words once everything is written out in full sentences. Here is an illustrative snippet of what part of this long prompt could look like when we put it together (for brevity, we&#x2019;ll just show a portion):</p>
<pre><code>**Your Role:** You are a senior education policy analyst tasked with reviewing and summarizing a comprehensive report on national education reform.

**Your Task:** 
Provide a structured, section-by-section summary of the 300-page report &quot;Education for the Future.&quot; Cover every major section of the report in your summary:
- **Introduction:** Summarize the context and purpose of the report.
- **Curriculum Reform:** Summarize the proposed changes to the curriculum and the reasons behind them.
- **Teacher Training Improvements:** Summarize findings and recommendations related to teacher professional development.
- **Infrastructure Investment:** Summarize any plans or recommendations for improving educational infrastructure (such as school facilities, technology, etc.).
- **Educational Outcomes:** Summarize the data or conclusions about student outcomes and any international comparisons.
- **Conclusion:** Summarize the final conclusions and any overarching recommendations of the report.

For each section, identify 2-3 key points or recommendations. Include any important data (statistics, case studies) mentioned in that section.

**Background Information:** 
The report was commissioned by the Ministry of Education in 2024, in response to declining test scores and widening rural-urban gaps in school resources. It includes data from a 5-year study involving 10,000 students and 1,000 teachers nationwide. Several pilot programs are referenced (e.g., a curriculum pilot in Province X that improved critical thinking skills by 15%, and a teacher training initiative in Province Y that reduced teacher attrition by 20%). International benchmark comparisons include Finland, Japan, and Singapore to contextualize the reform proposals.

**Writing Guidelines:** 
- Tone: Formal and objective. The summary should read like a briefing for policymakers.
- Length: Aim for about 1500-2000 words in total.
- Clarity: Use clear, direct language. Avoid jargon; if technical terms are used in the report, briefly explain them.
- Assume the reader has basic familiarity with education policy, but is looking for the high-level findings and implications.

**Important Requirements:** 
- Do **not** omit any major section of the report; ensure each of the five areas listed above is addressed.
- Do **not** add information that isn&#x2019;t in the report. If the report&#x2019;s details on a point are thin, state that briefly rather than guessing.
- Emphasize any data or evidence from the report to support each point (e.g., include the statistics provided).
</code></pre><p>Notice how exhaustive this prompt is. We&#x2019;ve spelled out <em>what</em> to do, <em>how</em> to do it, <em>what to cover</em>, and <em>what not to do</em>. We&#x2019;ve given context that the AI wouldn&#x2019;t otherwise know (like the key findings from the report). We&#x2019;ve set a clear structure and even listed the sections explicitly. This prompt is easily hundreds of times longer than our initial one-sentence query. It might feel almost like we&#x2019;re writing the summary ourselves&#x2014;but we&#x2019;re not writing the final text, we&#x2019;re writing instructions for the AI to do so.</p>
<p>Now, what kind of output do we get from a prompt like this? When the LLM receives this detailed directive, the difference in the quality of the response is dramatic. The model now has a blueprint. It knows it must go section by section, it knows the context, it knows the tone and length. It&#x2019;s not wondering, &#x201C;Should I talk about teacher training? Should I mention data?&#x201D; &#x2014; we&#x2019;ve already told it &#x201C;Yes, absolutely include those.&#x201D;</p>
<p>Continuing our example, here&#x2019;s an imagined excerpt from the AI&#x2019;s output when given the long prompt:</p>
<p><strong>Output from long prompt (excerpt):</strong>  </p>
<blockquote>
<p><strong>Introduction:</strong> The <em>Education for the Future</em> report (2024) was commissioned to evaluate and improve the national education system. It highlights challenges such as declining student performance and inequality between urban and rural schools, setting the stage for a comprehensive reform strategy.  </p>
<p><strong>Curriculum Reform:</strong> The report proposes a shift to a competency-based curriculum. It recommends updating learning standards to emphasize critical thinking, problem-solving, and creativity. For instance, a pilot program in Province X replaced rote learning with project-based learning, resulting in a 15% improvement in critical thinking assessment scores. The authors argue this approach will better prepare students for a modern economy.  </p>
<p><em>(... similar detailed summaries for the remaining sections: Teacher Training, Infrastructure, etc. ...)</em>  </p>
<p><strong>Conclusion:</strong> In conclusion, the reform report presents a roadmap for elevating the country&#x2019;s education system through targeted changes in curriculum, teacher development, and infrastructure. If implemented, these reforms aim to foster more equitable and higher-quality education outcomes, positioning the country to meet future challenges in education and workforce readiness. </p>
</blockquote>
<p>You can immediately see the contrast. The output from the long prompt is organized, thorough, and filled with specifics. It reads like an executive summary of the actual report. Each major section of the report is represented with its key findings, and even illustrative data (the 15% improvement, the attrition rate reduction, etc.) is included because our prompt made it clear to include those details. There&#x2019;s no hallucinated extra detail &#x2014; everything in the output was mentioned in the prompt&#x2019;s background (we gave the AI those numbers from the report&#x2019;s pilot programs as part of the context). The tone is formal and suitable for policymakers, just as we specified. And importantly, nothing was skipped: even if one section had fewer findings, the AI still noted whatever was relevant, because we told it not to omit any section.</p>
<p>In contrast, the short prompt version never mentioned those pilots, never gave statistics, and lumped everything into a broad generality. The long prompt version reads like a faithful summary of the actual 300-page document, whereas the short prompt version was closer to a generic placeholder.</p>
<p>This example demonstrates the core benefit of the Long Prompt pattern: <strong>when you provide rich detail and clear instructions, you guide the LLM to produce output that is far more useful and accurate</strong> than what you&apos;d get from a skimpy prompt. Yes, it takes more upfront effort to write such a prompt, but the payoff is significant. You spend that time once, and you get a high-quality result, rather than spending the time afterwards editing or trying multiple rounds of Q&amp;A to fix issues.</p>
<p>Before concluding, it&#x2019;s worth noting that long prompts can be applied to many contexts. We showed an example for summarizing a report, but the same principles apply to other tasks. For instance, if you were using an AI to draft an internal policy memo, you could provide the role (e.g., &#x201C;You are a policy advisor writing for the Minister&#x201D;), the context and facts to include, an outline of the memo&#x2019;s sections, and style guidelines for formal government communication. In fact, one successful prompt for writing an internal government brief included a thorough description of the writer&#x2019;s persona and audience (to set context) along with a detailed writing guide for the document&#x2019;s structure. As another example, journalists have used very long prompts to generate consistent news digests; the prompt would describe the editorial angle, give examples of desired output, and list criteria to check in the summary (such as ensuring certain perspectives are included). These cases all show the versatility of the Long Prompt pattern.</p>
<p>By now it should be clear why a long prompt (hundreds or even thousands of words) can yield such strong results. It&#x2019;s like giving the model a detailed map to follow, rather than just pointing in a general direction. As a user, once you see the difference, it&#x2019;s hard to go back to lazy one-liners for complex tasks. Writing a long prompt does require a bit of planning and thought, but don&#x2019;t be intimidated. The remainder of this book is dedicated to patterns and techniques that will help you develop and refine these prompts. You&#x2019;ll learn how to structure your instructions, how to incorporate examples effectively, how to iterate and polish your prompts, and more. With those tools in hand, you&#x2019;ll be able to make the Long Prompt a natural part of your approach&#x2014;when in doubt, be verbose and clear with your instructions. Use a Long Prompt, and you&apos;ll unlock the full potential of what the LLM can do for you. </p>

                                
                                </section>
                            
    </div>
    <div class="search-results">
        <div class="has-results">
            
            <h1 class="search-results-title"><span class='search-results-count'></span> results matching "<span class='search-query'></span>"</h1>
            <ul class="search-results-list"></ul>
            
        </div>
        <div class="no-results">
            
            <h1 class="search-results-title">No results matching "<span class='search-query'></span>"</h1>
            
        </div>
    </div>
</div>

                        </div>
                    </div>
                
            </div>

            
                
                <a href="../" class="navigation navigation-prev " aria-label="Previous page: Introduction">
                    <i class="fa fa-angle-left"></i>
                </a>
                
                
                <a href="prompt_file.html" class="navigation navigation-next " aria-label="Next page: Prompt File">
                    <i class="fa fa-angle-right"></i>
                </a>
                
            
        
    </div>

    <script>
        var gitbook = gitbook || [];
        gitbook.push(function() {
            gitbook.page.hasChanged({"page":{"title":"Long Prompt","level":"2.1","depth":1,"next":{"title":"Prompt File","level":"2.2","depth":1,"path":"catalog/prompt_file.md","ref":"catalog/prompt_file.md","articles":[]},"previous":{"title":"Introduction","level":"1.1","depth":1,"path":"README.md","ref":"README.md","articles":[]},"dir":"ltr"},"config":{"gitbook":"*","theme":"default","variables":{},"plugins":[],"pluginsConfig":{"highlight":{},"search":{},"lunr":{"maxIndexSize":1000000,"ignoreSpecialCharacters":false},"sharing":{"facebook":true,"twitter":true,"google":false,"weibo":false,"instapaper":false,"vk":false,"all":["facebook","google","twitter","weibo","instapaper"]},"fontsettings":{"theme":"white","family":"sans","size":2},"theme-default":{"styles":{"pdf":"styles/pdf.css","epub":"styles/epub.css","mobi":"styles/mobi.css","ebook":"styles/ebook.css","print":"styles/print.css","website":"styles/website.css"},"showLevel":false}},"structure":{"langs":"LANGS.md","readme":"README.md","glossary":"GLOSSARY.md","summary":"SUMMARY.md"},"pdf":{"pageNumbers":true,"fontSize":12,"fontFamily":"Arial","paperSize":"a4","chapterMark":"pagebreak","pageBreaksBefore":"/","margin":{"right":62,"left":62,"top":56,"bottom":56}},"styles":{"pdf":"styles/pdf.css","epub":"styles/epub.css","mobi":"styles/mobi.css","ebook":"styles/ebook.css","print":"styles/print.css","website":"styles/website.css"}},"file":{"path":"catalog/long_prompt.md","mtime":"2025-04-19T16:13:13.657Z","type":"markdown"},"gitbook":{"version":"3.2.3","time":"2025-04-19T16:13:44.653Z"},"basePath":"..","book":{"language":""}});
        });
    </script>
</div>

        
    <script src="../gitbook/gitbook.js"></script>
    <script src="../gitbook/theme.js"></script>
    
        
        <script src="../gitbook/gitbook-plugin-search/search-engine.js"></script>
        
    
        
        <script src="../gitbook/gitbook-plugin-search/search.js"></script>
        
    
        
        <script src="../gitbook/gitbook-plugin-lunr/lunr.min.js"></script>
        
    
        
        <script src="../gitbook/gitbook-plugin-lunr/search-lunr.js"></script>
        
    
        
        <script src="../gitbook/gitbook-plugin-sharing/buttons.js"></script>
        
    
        
        <script src="../gitbook/gitbook-plugin-fontsettings/fontsettings.js"></script>
        
    

    </body>
</html>

