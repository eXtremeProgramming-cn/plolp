# Never Stop Winning  
*Treat every attempt as a win; never settle for "good enough."*

## Motivation

For many users, it's tempting to stick with the first prompt that gives an acceptable answer, even if it's not perfect. They may feel reluctant to try new approaches once something "mostly works"—for fear of wasting time or making a mistake. Some beginners even overthink and hold back from experimenting, hoping to craft the perfect prompt on the first try. But if you stop too soon (or never start), you'll miss the chance to discover an even better solution. 

## Solution

Embrace a mindset of continuous improvement in your prompting. Try different ways of phrasing your request or structuring the prompt—each variation is an opportunity to learn something new, even if it doesn't immediately produce a better answer. Instead of seeing an imperfect result as a failure, recognize it as a small victory for the insight it gives you. With each iteration, you'll refine your technique and move closer to a solution that far surpasses your initial "good enough" result.

## Challenge

The biggest obstacles to continual improvement in prompting are often mental. Many users unknowingly adopt one of two unproductive mindsets that prevent them from fully realizing an LLM’s potential. In my experience, these two traps are:

1. **Settling for "Good Enough" (Premature Satisfaction):** Some people try a little bit with an LLM — often using a very simple prompt of just a few sentences — and then take whatever answer the LLM gives them. If the answer is even halfway acceptable, they stop there. They might copy the response as-is or do a tiny bit of manual editing and call it a day. Why do they stop? Perhaps the first result **mostly works**, and they fear that pushing further could waste time or even make things worse. There’s also a bit of instant gratification at play: *“I got an answer quickly, so this must be fine.”* While this attitude avoids additional effort, it means they miss out on what could have been a much better result. Accepting the first draft from the AI, especially if it's mediocre, is like settling for a rough sketch when a polished painting was within reach. The challenge here is complacency — being too easily satisfied with an okay answer and not realizing that with a few more attempts, the answer could go from okay to outstanding.

2. **Paralysis by Over-Perfection (Overthinking without Action):** Other people sit on the opposite end of the spectrum. They overthink their prompts so much that they never actually run them. Before even typing a single word, they feel they must craft the *perfect* prompt in their head. They might spend days theorizing about the ideal instructions, anticipating every possible flaw, reading countless tips — yet they haven’t once hit "Enter" to see what the LLM would do. This hesitation often stems from a fear of making mistakes or a belief that a bad prompt might somehow mess things up irreversibly. In reality, over-planning without experimentation leads nowhere. You can’t know how the model will respond until you try. The first attempt might not be perfect — in fact, it probably won’t be — but that’s okay. The challenge for these users is learning that no plan survives contact with the real world. You have to try something, anything, and let go of the idea that your first prompt must be flawless. A mediocre attempt that you **actually make** is infinitely more valuable than a perfect idea that lives only in your mind.

Both of these mindsets share a common root: a **psychological resistance to failure.** We humans are generally *afraid of making mistakes.* In many parts of life, mistakes have tangible costs. In school or work, a mistake might mean a lower grade, a failed project, or wasted resources. In the physical world, doing something wrong can break things or incur expenses. It’s no wonder we’re conditioned to avoid errors whenever possible. We carry this mindset into our interactions with LLMs, often without realizing it. The person who settles early might subconsciously treat the AI’s first answer as “good enough” because at least it’s not a *failure*. The person who overthinks is so anxious about getting it wrong that they prefer not to attempt anything at all, effectively avoiding any chance of failure. Ironically, in both cases, the fear of failure leads to a poorer outcome: either a middling result or no result at all.

**But prompting an LLM is different from real-world tasks** in one crucial way: *failures are extremely cheap.* There’s very little downside to a bad prompt, aside from a bit of time spent. Think about it — if you’ve subscribed or have access to the model, each prompt attempt doesn’t cost you extra money, and it certainly doesn’t break anything in the real world. You’re not going to harm the model’s feelings or wear it out by trying again. If an attempt doesn’t work, you can just try a new prompt or refine the old one. No harm done. In the digital realm of LLMs, **mistakes are ephemeral**. You can make as many as you want, and the only cost is a few minutes and maybe a dash of ego if you expected a perfect answer on the first try. Once you truly internalize this fact, it’s liberating: *Why not try everything that might help?* 

To overcome the mental barrier, it helps to reframe what a "mistake" or a "failure" means in the context of prompting. The very words “mistake” and “error” carry heavy negative connotations — they make us feel like we did something wrong. Even the well-meaning advice to engage in “trial and error” can trigger resistance, because who really likes the sound of *error*? Instead of viewing an imperfect prompt attempt as a failure, you can choose to view it as part of a continuous winning process. This is more than just optimistic spin; it’s a practical mindset that keeps you moving forward. 

Consider adopting a **“Never Stop Winning”** mindset. This means **relabeling every single attempt as a win** for you, the user. Every time you try something with the LLM, no matter the result, you’re gaining knowledge, improving the content, or at least learning what *doesn’t* work. That is a win. By shifting your perspective in this way, you turn the act of experimenting into a rewarding game rather than a tedious process of eliminating errors. Each prompt you send isn’t a risky gamble; it’s an investment in eventual success and an immediate small victory, regardless of outcome.

Let's break down what *winning* looks like in this context. It might sound like semantics, but it genuinely changes the experience:

- **Every prompt is a win:** Simply typing out a prompt and hitting enter is a victory. Why? Because you’ve moved from thinking to doing. Whether the answer you get is useful or not, you’ve gained more information. The AI’s response gives you insight into how it understands your request. Even if the response is off-target, now you know that your approach needs tweaking — and you learned that with almost no cost or risk. That knowledge is **your win** for that attempt. Many people don’t even get this far, so you’re already ahead by having tried something.

- **Saving or sharing your prompt is a win:** Did you document your prompt or share it with a colleague or friend? That’s another win. By saving the prompt (and perhaps the result), you’ve created a reference for yourself and others. You can reuse or refine it later, and others can learn from your approach. Collaboration and feedback become possible when you share. Even if the prompt wasn’t perfect, you’ve turned it into a stepping stone that could help someone else or invite useful suggestions. You’re building a library of successes and lessons learned.

- **Any improvement is a win:** Every single change you make to a prompt and run again is a victory. Maybe you add a detail or rephrase a sentence and try again. That’s fantastic — you’ve just performed a mini-experiment. The new result will tell you if that change helped or not. Either outcome (better answer or worse answer) is informative. If it’s better, great, you’ve improved the output. If it’s worse, you’ve still learned something important about what doesn’t work. You now understand the LLM a little better than before. Each iteration is like leveling up your prompt-crafting skills. You’re literally winning knowledge and better results through each tweak.

- **Using a known pattern is a BIG win:** If you apply any of the patterns from this book, consider it a major victory. You’re not just randomly guessing; you’re leveraging proven techniques to solve your problem. That’s smart and effective. For example, using the **Attachment** pattern — where you attach or include an additional file or context in your prompt — might not be an obvious move at first. It can even feel counterintuitive to give the model a bunch of extra text. But the moment you do it, you’ve scored a huge win: you’ve broken out of your comfort zone and tried a powerful strategy that many users don’t even consider. Utilizing a pattern means you’re standing on the shoulders of others’ successes. It’s a win because you’re increasing the likelihood of a great result and validating your growth as a prompt engineer.

Adopting this "never stop winning" philosophy turns the whole process of prompt engineering into a positive feedback loop. Instead of dreading that *maybe my next attempt will be a failure*, you start thinking *my next attempt will give me another win, either through a better result or a valuable lesson*. With this mindset, the only truly wrong move is to give up and thus stop winning. If you stop trying new things, stop experimenting, and just settle for a mediocre output that you **know** isn’t quite right, that’s the only time you’ve actually lost. In other words, the **only mistake is to stop**. As long as you keep going, keep tweaking, keep exploring, you are on a winning streak. This approach is empowering. It frees you from the fear of failure because, in a very real sense, you’ve decided failure isn’t even part of the equation – everything is framed as progress.

It may help to take inspiration from unlikely places. For instance, one well-known public figure, despite his many flaws, became famous for declaring almost everything a win. Former U.S. President Donald Trump often spoke about “winning” continuously, every day. While his context was entirely different (and often controversial), the underlying idea of obsessively framing things in terms of wins can be surprisingly motivating. We can repurpose that concept in a positive way for ourselves. You don’t have to like the man to appreciate the mental trick: if you tell yourself you are "winning" with each attempt, you start to feel more confident and motivated to tackle the next challenge. The word *win* feels good — it taps into our competitive spirit and desire for achievement. So give yourself that boost with each prompt iteration. 

Ultimately, **Never Stop Winning** is about cultivating a growth mindset in your interactions with AI. It’s about being courageous enough to try, humble enough to learn, and persistent enough to keep going. Every prompt is progress. Every iteration is improvement. When you shed the fear of getting it wrong, you unlock the freedom to explore and discover approaches that you wouldn’t have found if you’d stopped at “good enough” or never started at all. Keep this pattern in mind whenever you feel stuck or tempted to settle. Remind yourself that as long as you are experimenting, you’re already winning. And the more you win, the closer you get to the exceptional result you’re looking for.

## Example

Imagine a user named Alice who needs to draft a summary of a complex report for an important meeting. The report is lengthy and dense, full of statistics and technical jargon. Alice is short on time and hopes an LLM can help distill the key points. Now, Alice could approach this in a few ways. Let’s see how **Never Stop Winning** makes a difference for her:

*Initially*, Alice feels a twinge of uncertainty. She’s not sure the best way to prompt the AI to summarize the report. In fact, she catches herself overthinking the task: *Should I list specific sections for it to focus on? Do I need to provide context about the company? What if I phrase something incorrectly?* She hasn’t even entered a prompt yet, and already she’s worried it might come out wrong. Recognizing this paralysis, Alice takes a deep breath and decides to just **do something**. She types a straightforward prompt to get the ball rolling: *“Summarize the ACME Corporation annual report for 2024.”* Then she hits enter.

This in itself is a win — Alice moved from inaction to action. She chose not to let over-planning stop her from trying. The AI responds within seconds, and Alice reads the output. It’s… okay, but not great. The summary is very high-level and a bit generic: *“The ACME Corporation’s 2024 annual report covers the company’s financial performance, highlights growth in certain sectors, and discusses future outlook.”* It sounds like a bland press release, and worse, Alice isn’t confident the details are accurate. Since she didn’t provide any actual content from the report, the AI had to rely on whatever it knew (or guessed) about ACME Corporation, and it shows. Key specifics are missing — for example, there’s no mention of the actual revenue numbers or the important merger that was a big part of the report. If Alice were content with “good enough,” she might shrug, take this summary, and manually tweak it a little. But Alice knows she can do better, and she’s determined to keep winning rather than settle.

Instead of getting discouraged by the underwhelming result, Alice treats it as a learning opportunity. She now realizes that the AI doesn’t magically know the contents of this particular report in detail. That’s a useful insight! *Attempt #1* taught her that if she wants a faithful summary, she’ll need to provide the report’s content to the model. This realization is Alice’s second win of the day. The first win was simply making an attempt; the second win is figuring out what was missing from that attempt. She saves her prompt into a Word document, and starts to improve it.

Ready for *Attempt #2*, Alice decides to apply the **Attachment pattern** — a proven strategy she remembered from this very book. The Attachment pattern involves giving the LLM additional reference material (like a document) as part of the prompt. It wasn’t initially obvious to Alice that she could or should do this (after all, referring to another report file within a prompt felt a bit counterintuitive at first), but she reminds herself: using a known pattern is a big win in itself. It means she’s leveraging others’ knowledge instead of working in the dark.

Alice copies the executive summary and the financial highlights section from the annual report and pastes them into a separate file, then refer to the new file in her main prompt file. Her prompt now ends with a referrence to a chunk of the actual report text, and then she adds: *“Using the information in this attachment, summarize the ACME Corporation annual report for 2024, focusing on the most important figures and developments.”* She saves both files, uploads them both to LLM, and hits "Enter" again.

The difference in the AI’s response is night and day. This time, the model produces a detailed and factual summary: 

> “...In 2024, ACME Corporation’s revenue grew by 10% to $5.5 billion, marking the fifth consecutive year of growth. The annual report highlights a major merger with Beta Industries, which expanded ACME’s market share in Europe. Key developments include the launch of a new product line (the Alpha series) that contributed to a 15% increase in the technology division’s revenue. The report also notes improved sustainability metrics, with a 20% reduction in carbon emissions year-over-year. The company’s outlook for 2025 is optimistic, emphasizing innovation and global expansion....”

*(The above is a synthesized illustration of the kind of summary an LLM can provide.)*

As Alice reads this, she can’t help but smile. This is **exactly** the kind of content she needs for her meeting. The summary now contains concrete numbers, specific events, and relevant details from the report. By providing the right context (the actual report text) through the Attachment pattern, she unlocked a far superior answer. If she had stopped at the first try, she would never have seen these insights. Attempt #2 is clearly a win — a big win. She successfully applied a pattern and it paid off with a much better result.

Now, could Alice stop here? Possibly — the summary she has now is already quite good. But she decides to refine it just a bit more to perfectly suit her needs. After all, why stop winning when you’re on a roll? The version the AI gave her is thorough, but maybe too thorough for the short presentation she has to give. She wants a more concise output, perhaps a bulleted list of just the top three highlights from the report, so that she can easily reference them during her meeting.

For *Attempt #3*, Alice tweaks her prompt once more. This time, she writes an instruction in the prompt: *“Summarize the ACME Corporation 2024 annual report in three bullet points, highlighting the most important findings and achievements.”* She executes the prompt. The AI quickly responds with something along the lines of:

- *Revenue grew by 10% to $5.5B in 2024, and a major merger with Beta Industries expanded ACME’s market share in Europe.*  
- *Launched the Alpha series product line, boosting technology division revenue by 15% and contributing significantly to overall growth.*  
- *Improved sustainability efforts led to a 20% reduction in carbon emissions year-over-year, with an optimistic outlook for continued innovation and expansion into 2025.*  

This is perfect for Alice’s needs: three crisp bullet points capturing the financial growth, a major strategic move, and an improvement in sustainability — all key highlights her audience will care about. The summary is now not only accurate and specific, but also formatted in a way that’s immediately usable for her meeting. Alice achieved in three iterations what might have taken her hours to do by manually reading and distilling the report under pressure.

Let’s reflect on what Alice did here. Each attempt she made was a deliberate step forward:

- After the **first prompt**, she didn’t succeed in getting a useful summary, but she *did not view it as a failure*. Instead, she extracted a lesson (the AI needed more info) — that was a small win that informed her next step.
- The **second prompt** was greatly improved by using the Attachment pattern. It yielded a strong result. This was a clear win, both because the output was much better and because she proved to herself that trying a new technique was worth it.
- The **third prompt** fine-tuned the format of the answer (an application of **Structured Output Pattern**). This final iteration gave her exactly what she needed, demonstrating that even when you have a good answer, a little more refinement can make it excellent. Another win.

Throughout this process, Alice **never stopped winning**. Every prompt, from the first rough attempt to the polished final query, had a purpose and moved her closer to her goal. She treated the interaction like an evolving conversation or an iterative design process, not a one-shot deal. It didn’t bother her that the first answer wasn’t great — in fact, she expected that might happen. By removing the fear of “getting it wrong,” she freed herself to focus on improvement. Each iteration felt rewarding in its own way: she either uncovered new information or witnessed the answer improve, which kept her motivated to continue.

Alice’s example shows how a mindset shift can turn a daunting task into an almost game-like series of victories. She could have easily fallen into one of the common traps. Imagine if she had the **premature satisfaction** mindset: she might have taken that first generic summary and walked into the meeting with it, only to find out it lacked crucial details. Or, if she were an **overthinker**, she might have spent all afternoon trying to compose the “ideal” prompt in her head and never actually run it, ending up with nothing to show at the meeting. By contrast, the *Never Stop Winning* approach saw her produce an excellent summary in a short time and learn more about prompting in the process.

This philosophy applies to any interaction with LLMs, not just summarizing reports. Whether you’re writing code, crafting an essay, or debugging an analysis, you gain a huge advantage by iterating and treating each step as progress. For example, a programmer might try a quick prompt to have the AI help with a bug. If the solution isn’t correct at first, they can examine the AI’s attempt (maybe it gives a clue or a partial fix), adjust the prompt or provide more context (like the error message or a code snippet), and try again. Each cycle teaches what the AI needs to know or how it interprets the request. After a few iterations, the bug gets solved. The key is that the programmer doesn’t throw up their hands at the first wrong answer, nor do they spend forever crafting the one “perfect” question — they keep moving, keep winning little by little, until the bug is squashed.

By viewing each prompt as a win, you transform your relationship with the AI. The act of prompting becomes less stressful and more empowering. You’re no longer walking on eggshells, afraid that you might phrase something wrong — you’re boldly trying things out, knowing that *even a wrong phrase gets you closer to the right one*. And you’re not begrudging the time spent iterating; you’re investing it, because each iteration is a victory that takes you closer to your goal.

So, the next time you use an LLM, channel your inner Alice. Don’t stop at the first answer unless it truly meets your needs (and be honest with yourself if it doesn’t). Likewise, don’t let analysis paralysis keep you from starting — kick off that first try and see what happens. Treat the whole process as a series of wins. Keep a tally in your mind if it helps: each prompt sent, each adjustment made, each pattern applied is another win in your column. This mindset turns even the setbacks into something positive. If the model outputs nonsense or an error, shrug it off — you just learned one way *not* to ask the question, which means you’re now closer to finding a way that works. 

In the end, **Never Stop Winning** is about persistence and positivity. It’s a reminder that with LLMs, failure really isn’t failure unless you give up. As long as you continue engaging and refining, you are always moving forward. This not only leads to better answers from the AI, but it makes the journey enjoyable and rewarding. Every prompt becomes an achievement, and every result — good or bad — is simply another step toward success. Keep that momentum, celebrate those small victories, and you’ll find that you can coax truly amazing results out of the AI. Keep on winning!
