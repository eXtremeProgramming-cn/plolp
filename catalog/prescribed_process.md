Great. I’ll expand the pattern brief from the file "prescribed_process.md" into a full book chapter following the instructions in "Prompt to Write One Pattern v1.2". This will include a clear and engaging explanation of the pattern, detailed challenge and example sections, and use a style accessible to non-technical users.

I’ll let you know as soon as the chapter draft is ready for your review.

# Prescribed Process  
*Outline steps to ensure thoroughness and adherence to the required workflow.*

## Motivation  
For complex tasks that involve in-depth research, multi-stage analysis, or detailed synthesis, simply asking the LLM for a final answer often isn’t enough. Without specific guidance, the model might take shortcuts, skip crucial steps, or follow a suboptimal approach. This can lead to results that are incomplete, shallow, or inaccurate because the process to reach the answer wasn’t carefully guided.

## Solution  
Provide the LLM with a clear, step-by-step game plan. In practice, this means outlining a specific process or sequence of actions for the model to follow. For example, you might:  
- **Divide the task into sequenced steps:** Present numbered instructions that break the work into logical stages (e.g., "1. Read the case study... 2. Analyze the data... 3. Draft the summary...").  
- **Include reference materials or guidelines:** If there are external documents, examples, or methodologies the model should use, mention them at the appropriate steps so the LLM has the right context or method for each part of the task.  
- **Set checkpoints or questions:** Insert checks during the process (e.g., "After analyzing the data, list the three most significant findings.") to ensure the LLM pauses and covers all key aspects before moving on to the next step.

By prescribing a procedure in the prompt, you guide the model’s reasoning and workflow. Instead of figuring out how to approach the task on its own, the LLM follows your roadmap, which helps ensure thoroughness and alignment with the expected approach.

## Challenge  

Have you ever asked an AI assistant a complex question and felt the answer missed something important? This is a common frustration when using LLMs for multi-faceted tasks. The core challenge is that if you don’t tell the model *how* to work through the problem, it will try to guess the method itself — and it might not guess right. Users often assume the AI will “just know” how to handle a complicated request, but in reality, the model lacks a defined process to follow. It's as if you hired a new intern and simply said, "Write the annual report," without any further guidance. Chances are you’d get an unsatisfactory report. The intern isn’t lazy or unintelligent; they just weren’t given a plan. Similarly, an LLM without a clear process can produce output that is off-target or underdeveloped.

**Why do LLMs skip steps?** Large language models generate answers based on patterns in their training data. If your prompt asks for a complex outcome but doesn’t specify the intermediate steps, the model may take a leap directly to an answer that *sounds* plausible. It won’t necessarily stop to double-check facts, consider alternative angles, or ensure each part of the question is addressed unless you prompt it to do so. The AI isn’t deliberately cutting corners; it’s following the path of least resistance — the shortest route to a seemingly acceptable answer. In a way, the model is doing what it has been rewarded for in the past: providing a fluent answer quickly. Without guidance, "quickly" can trump "comprehensively."

Consider a scenario where you ask an LLM: *"Evaluate the impacts of a new city transportation policy and summarize your findings."* If this request is given as a single-shot prompt, the model might produce a general summary of the policy’s impacts. However, it might gloss over important details: perhaps it mentions a few obvious pros and cons but misses less obvious factors like long-term environmental effects or public sentiment data. Why? Because we didn’t ask it to perform the underlying steps explicitly. In its one-paragraph answer, the model implicitly tried to do an entire analysis in one go — something even a human expert would normally do in stages (research the policy background, analyze data or case studies, weigh different perspectives, then summarize). Without us outlining those stages, the LLM’s response ends up superficial.

From the user’s perspective, this is frustrating. You expected a thorough analysis, but got a cursory overview. A non-technical user might not immediately realize that the *prompt design* is the culprit here. It’s natural to assume that saying *"evaluate and summarize"* would lead the AI to actually carry out an evaluation process internally. But often, the model doesn’t truly **evaluate** in the analytical sense; it just generates text that looks like an evaluation. The critical thinking steps — like gathering evidence or systematically comparing outcomes — might never have occurred.

Now imagine the same user tries the request again, but this time breaks it down: *"1. List the key areas affected by the new transportation policy (traffic, economy, environment, etc.). 2. For each area, briefly research or discuss the impact based on available information. 3. Compile these findings into a summary of overall impacts."* This prompt explicitly instructs the model how to tackle the problem. The difference in the resulting answer is immediately noticeable: the LLM provides a structured analysis, covering traffic, economy, environment one by one, citing relevant points for each, and then gives a concluding summary that ties them together. In this second attempt, nothing magical was added to the model — we simply changed the way we asked. By prescribing the process, we got the model to perform a more diligent analysis.

This example highlights the challenge that leads to the *Prescribed Process* pattern. Users need completeness and accuracy, but the LLM won’t inherently know which steps matter to us. **Without guidance, an AI might overlook critical requirements.** For instance, if a civil servant asks an LLM to "draft a comprehensive policy brief on improving digital literacy programs," a lot could go wrong without a defined process. The model might churn out a decently written brief, but maybe it forgot to include current statistics or failed to address certain stakeholder perspectives. Perhaps it didn’t cover an important section like "Implementation Challenges" because the prompt never explicitly mentioned it, and the model’s best guess at what a "policy brief" should include was incomplete.

Such omissions are not because the AI is intentionally ignoring instructions; it's because the instructions were not detailed enough. The user essentially gave a *declarative* command: they described the desired end product ("a comprehensive policy brief") but not the method to get there. A declarative prompt leaves the "how" up to the AI. If the AI has seen many policy briefs, it might do an okay job. But if the specific context or thoroughness we need is unique, the AI’s generic approach will fall short. It’s the classic difference between **declarative** and **instructive** approaches. A declarative request is outcome-focused ("I want this result"), whereas an instructive request is process-focused ("Do these specific things to achieve the result"). Neither approach is inherently wrong — declarative prompts can work for simpler or very well-defined tasks — but for complex and high-stakes tasks, purely outcome-focused prompting is risky.

Another challenge is that many users don’t initially realize they *can* direct the AI with detailed steps. It’s easy to treat an LLM like a search engine or an encyclopedia: you throw a question at it and expect a useful answer. But an LLM is more like a very talented, very eager intern. If you give it a vague assignment, it will do *something*, but the quality will vary. The intern might produce a decent draft using common knowledge, but miss nuance and depth — exactly what an LLM does when it generates the most statistically likely response. To get great results, you often need to be more like a coach or project manager: set out the tasks, one by one.

There's a learning curve for the user here. Breaking down a task into a formal process requires thinking through the problem yourself. Non-technical professionals might worry, "I’m not a programmer — I don’t know how to 'tell the computer what to do' in steps!" Fortunately, writing a step-by-step prompt doesn’t require programming skills, just a clear idea of what needs to happen. In fact, if you know your domain or task well, you’re the best person to outline the approach. The challenge is mostly mental: pausing and planning the task structure before asking the LLM to execute it. Many of us are used to jumping straight into writing or asking for an answer, so it can feel unnatural to step back and outline a procedure first. But this extra effort can pay off dramatically in the quality of the AI’s output.

Let's address a potential concern: **won’t step-by-step instructions limit the AI’s creativity?** In some situations, you might worry that telling the model exactly how to work could constrain it. This is a valid consideration. If you over-constrain the process for a task that actually benefits from a bit of freeform creativity, you might get an overly rigid result. The key is to use *Prescribed Process* when you need thoroughness and correctness more than out-of-the-box creativity. Even then, you can leave some steps open-ended to let the model contribute ideas. For instance, one step in your prompt could be "Brainstorm at least three innovative solutions to the problem," which gives the AI room to be creative within a guided framework. In general, creativity in LLMs can still flourish inside a structured prompt — the structure just ensures that the creativity is applied to every part of the problem, not just the part the model happens to focus on first.

Another subtle challenge is knowing how detailed your instructions should be. If you specify every micro-step, the prompt could become unwieldy or the model might get bogged down. If you’re too high-level, the model might misinterpret your intentions. Finding the right granularity comes with practice. A good rule of thumb is to imagine explaining the task to a human who is smart but unfamiliar with the specifics. You wouldn’t micromanage things they clearly know how to do, but you would outline steps for the complicated or crucial parts. The same applies to instructing an AI. 

Finally, technology plays a role: current LLMs have some limitations in following very complex or conditional processes perfectly. If you say, "for each of these 10 items, do X, then if Y happens, do Z," a language model might not flawlessly execute that kind of loop or conditional branching, because it doesn’t truly have a programming loop structure. It will try to follow the instruction, but complex logic can lead to confusion in the output. As of 2025, advanced models are improving at handling more intricate instructions, but you still want to keep processes straightforward. The *Prescribed Process* pattern doesn’t mean you can code an entire algorithm into a single prompt reliably (at least not yet); it means you guide the AI through a reasonable series of steps to maximize completeness. If the task truly requires branching logic or heavy iteration, it might be better handled with multiple prompts or a different approach. But for the vast majority of tasks a civil servant or other professional might face — like writing reports, analyzing scenarios, summarizing information, or comparing options — simply enumerating the main steps will hugely increase the quality and reliability of the output.

In summary, the challenge that *Prescribed Process* addresses is the gap between what we imagine the AI will do on its own and what it actually does. Without guidance, the AI might deliver a result that only partially meets your needs, forcing you to spend extra time fixing or completing the work yourself. This pattern recognizes that effective prompt design often means spelling out the journey, not just the destination. It shifts the effort to the beginning (crafting a good process in the prompt) rather than the end (having to heavily edit or redo the AI’s output). Once you overcome the initial habit of issuing only broad prompts and start providing a roadmap, you’ll find the AI can tackle surprisingly complex projects in a structured way.

## Example  

To see how *Prescribed Process* works in practice, let’s walk through a concrete scenario. Suppose you are a policy analyst tasked with writing a briefing on improving digital literacy in public schools. You need this briefing to include an overview of current digital literacy levels, challenges faced by schools, successful case studies from other regions, and some practical recommendations. That’s a lot of ground to cover. We’ll explore two approaches to this task: first a naive one-shot prompt, and then a prescribed process prompt, to highlight the difference.

*Naive approach (for contrast):* You might start by giving the LLM a straightforward request like, "Write a comprehensive policy brief on ways to improve digital literacy in public schools." This is a single-sentence, declarative prompt. The model will try its best: it might produce a few paragraphs discussing general points—perhaps touching on the importance of teacher training, suggesting adding computers in classrooms, and mentioning vaguely that "some programs have been successful elsewhere." The style might be fluent and it may look okay at first glance. However, upon closer inspection, you notice it’s lacking specifics: Where are the statistics about current literacy rates? It briefly notes "successful programs" but doesn’t name any. It doesn't mention challenges like budget constraints or varying skill levels among teachers. In short, it’s generic. This happened because the AI was not guided to gather data or examples; it just pulled from its general knowledge to fill the request in a generic way.

Now, let’s apply the *Prescribed Process* pattern to the same task.

*Guided approach with Prescribed Process:* We’ll craft a prompt that breaks the briefing into a series of explicit steps for the LLM to follow. For instance:

> **Prompt:**  
> 1. **Background** – Summarize the current state of digital literacy in public schools. Include any statistics or reports that illustrate the level of digital skills among students and teachers, and note any relevant national standards or goals.  
> 2. **Challenges** – Identify and explain the key challenges schools face in improving digital literacy (for example, lack of access to technology, insufficient teacher training, curriculum gaps, funding issues). Provide at least a sentence or two on each major challenge.  
> 3. **Case Studies** – Briefly describe two successful initiatives or programs (could be from other cities or countries) that improved digital literacy in schools. Include what they did and the results they achieved.  
> 4. **Recommendations** – Propose 3-5 actionable recommendations for improving digital literacy in our public schools, based on the challenges and lessons from those case studies. Make sure the recommendations are specific and realistic (e.g., "implement a teacher training program in digital skills" rather than just "improve teacher training").  
> 5. **Conclusion** – Conclude the briefing by summarizing how these actions can collectively raise digital literacy, and maybe suggest next steps or highlight the importance of taking action.

Look at what we’ve done here. The prompt is structured with clear sections (Background, Challenges, Case Studies, Recommendations, Conclusion). Each section has a specific goal and even some hints about what to include. We’ve essentially given the LLM a mini-outline that resembles how an experienced human might approach the report. The model doesn’t have to wonder what the outline of the brief should be – we’ve handed it one. Also, note that within these steps, we nudged the model to include certain details: statistics in the background, at least a sentence on each challenge, an example in the case studies, multiple items in recommendations. These act as checkpoints. They ensure that the AI doesn’t gloss over a part by giving just a cursory mention and moving on. If the model follows the instructions, it will produce content for each bullet, yielding a thorough briefing.

When the LLM processes this prompt, the output it generates will likely mirror the structure we provided:  
- It will start with a Background section, perhaps stating something like, "Currently, only X% of students demonstrate proficient digital literacy skills according to [a recent study], and teachers report limited training in integrating technology into classrooms...". This immediately anchors the report with factual context.  
- Next, under Challenges, the response will enumerate issues such as "Access to Technology: Many schools, especially in under-funded districts, lack sufficient devices or high-speed internet..." and so on, covering each challenge in turn. Because the prompt explicitly asks for key challenges, the model is less likely to omit one. If it knows common issues (and it does, from its training data), it will try to list several—which is exactly what we want.  
- Under Case Studies, you’ll see the model describe a couple of concrete examples, maybe "Case Study 1: A rural county launched a 'Digital Bus' initiative that traveled to schools... resulting in a 20% increase in student digital proficiency after one year," and "Case Study 2: A city partnered with a tech company to train teachers in coding, leading to ...". These examples make the briefing far more insightful. We got them because we specifically told the AI to include case studies. Without that step, nothing in the one-shot prompt compelled the model to include real examples.  
- The Recommendations section will lay out specific actions, like "Provide Annual Teacher Training in Technology: Allocate funding for yearly workshops to keep teachers updated on digital tools and teaching methods..." and a few more such recommendations. Notice how each recommendation ties back to the challenges (a good model will do this naturally given the context we set). We effectively prompted a more thoughtful response by saying "3-5 actionable recommendations." The AI knows it shouldn’t stop at one or two, and that each should be concrete.  
- Finally, the Conclusion will tie the pieces together, perhaps reiterating that addressing these areas could significantly improve students’ digital skills and better prepare them for a technology-driven world, while calling attention to the importance of taking these actions.

The resulting content is much richer and more organized than the output from the one-sentence prompt. By outlining what to do in each part, we ensured the LLM didn’t forget any major component. A nice side effect: the structured output is easier to read. It has clear headings or at least logical divisions, which is great if you plan to use this briefing or share it, because it reads like a well-structured document an expert might have written.

This example is fictitious, but it mirrors real experiences users have with guided prompts. In practice, using a prescribed process can be the difference between a mediocre draft and a solid first draft that only needs minor polishing. A civil servant in the education sector (our prototypical reader) could apply this same pattern to countless tasks: writing a grant proposal (by outlining sections like "Needs Analysis, Objectives, Methodology, Budget, Evaluation Plan"), preparing meeting notes (by listing agenda items and summarizing the discussion for each), or analyzing survey results (by instructing the AI to first tabulate responses, then identify key trends, then write a summary of findings). The specifics change with the task, but the principle remains: break the job down and tell the AI the steps to go through.

While applying the *Prescribed Process* pattern, here are a few practical tips:  
- **Use numbering for clarity:** Numbered lists in your prompt help the AI see the distinct tasks. It also allows you (or the AI) to reference steps if needed (for example, you could say, "Refer back to the insights from step 2 when writing the conclusion in step 5").  
- **Keep steps logically distinct:** Each step should have a clear purpose or focus. If you find yourself writing a run-on instruction with multiple "and then..." clauses, consider breaking it into separate steps. The cleaner the separation, the less likely the model will merge or skip tasks.  
- **Add detail to steps if necessary:** You can include sub-bullets or parentheses to clarify what a step entails. In our example, we added hints in parentheses (like suggesting including statistics or ensuring recommendations are specific). This guides the depth and direction of each step.  
- **Be mindful of prompt length:** Very long prompts with too many detailed steps can sometimes overwhelm the model or hit token limits. Aim to include all crucial steps, but avoid overloading on minutiae. Think of it as giving the AI a checklist of "must-do" items rather than an exhaustive script. If a task is extremely complex, you might implement it with a few separate prompts or refine your steps to the most essential actions.  
- **Test and refine:** After the AI generates an output using your process, review the result. Did it follow every step correctly? If something was missed or done poorly, refine that part of your prompt and try again. Over time, you’ll get a feel for how the AI interprets your instructions. Perhaps you learn that the AI misunderstood "briefly describe" to mean one sentence, so next time you might say "describe in one or two paragraphs." This iterative refinement is part of developing effective prompts.

To conclude, *Prescribed Process* is a powerful pattern whenever you need an LLM to tackle a task that has multiple parts or requires careful execution. By spelling out the steps, you’re effectively teaching the model *how to think* about your problem, not just *what to say*. This reduces the chance of errors or omissions and increases the likelihood of getting a useful, well-structured result on the first try. It transforms the prompt from a simple question into a guided plan of action. For the non-technical professional, adopting this approach can make the difference between feeling like "the AI didn’t get it right" and "the AI became a valuable assistant that did exactly what I needed."
