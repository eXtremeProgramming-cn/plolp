Thanks for clarifying! I’ll now expand the 'Relevance' pattern brief into a full chapter, using the required structure and tone.

I’ll ensure it emphasizes how to explicitly indicate the importance (or unimportance) of each part of a prompt’s input—especially when using Prompt Files and Attachments—and includes a practical example with attachments.

I’ll let you know once the draft is ready for review.

# Relevance
*Always clarify how each input is relevant to the task.*

## Motivation
Providing a lot of information to an LLM (through text, attachments, or search results) without context or guidance can end up confusing the model. The AI might not understand which details are crucial and which are secondary. It could either ignore something important or include something irrelevant simply because it was mentioned in the prompt. This lack of direction can lead to responses that miss key points or stray into unrelated topics.

## Solution
Whenever you include a piece of information in your prompt, explain why it’s there or how it should be used. In other words, explicitly highlight the relevance (or irrelevance) of each input to the main task. For instance, you might annotate your prompt file with notes like: “Document A provides background on the policy (for context),” or “Dataset B is unrelated to the core question and is included just for completeness.” By clearly stating the role of each part of the input, you help the LLM focus on what truly matters and prevent it from being misled by extraneous details. In short, don’t assume the model will infer why something is included—spell it out.

## Challenge
Including multiple sources, documents, or background information in a prompt without indicating their relevance poses several challenges for both you and the AI. When you dump a pile of information into your prompt and hit “go,” the model is left to guess how it all fits together. Unfortunately, the AI is not a mind-reader. It doesn’t inherently know why you provided each piece of content. And unlike a human colleague, it won’t ask you for clarification – it will simply forge ahead with its own assumptions. This can result in a few common problems:

- **Irrelevant details creep into the answer:** If you don’t tell the model what’s important (and what isn’t), it may decide that everything is fair game. The AI could incorporate tangential facts or anecdotes from your attachments just because they were present. *For example, a policy analyst might attach a 10-page background report along with their question, hoping to give the AI context. Because they never pointed out which parts of that report were important, the AI ended up pulling in a trivial detail from deep in the document (a sidebar about a minor issue) just because it was included. The analyst’s final answer wandered off-topic, including information that had little to do with the actual question.*  This isn’t the model being malicious; it’s just trying to use everything you gave it, lacking any hint of what to prioritize or ignore.

- **Crucial points get overlooked:** The flip side of irrelevant details showing up is that important information can be ignored when its significance isn’t highlighted. *Suppose a team lead includes two documents in a prompt: one is a short list of key requirements, and the other is a long appendix of background data. The team lead doesn’t emphasize that the short list contains the critical points. The AI, treating both inputs equally, gets bogged down in the appendix details and glosses over the actual requirements. The resulting output, despite all the information given, misses the main point the team needed.* In this case, because the key points weren’t flagged as important, the model’s answer ends up focusing on the wrong material.

- **Mixed messages cause confusion:** When multiple sources are presented without explanation, the model can get confused about what you actually want. *Imagine a manager asks an AI to draft a policy memo and attaches three files: last year’s memo (as a reference), some raw survey data (for evidence), and an unrelated example document that accidentally got included. With no instructions about these attachments, the AI tries to use them all. The draft it produces is an odd mash-up: it copies some text from last year’s memo, sprinkles in a few out-of-context figures from the data, and even references the unrelated document. None of this was the manager’s intent, but the AI had no way to know that.* The lack of clear signals from you means the model is essentially guessing at what’s relevant, often with muddled results.

- **Conflicting information without guidance:** If the materials you provide contain discrepancies or opposing points, and you don’t clarify which to prioritize, the AI can become uncertain or give a contradictory answer. *For example, an analyst provides an older policy document and a newer update on the same topic, but doesn’t explain that the newer one supersedes the old. The confused AI might combine statements from both or arbitrarily choose details from each. In one case, the AI’s answer actually presented outdated information from the old policy as if it were current. The analyst received a contradictory result simply because the model wasn’t told which source to trust.* Without guidance on how to resolve the conflict, the model’s best guess may not align with what you actually need.

- **User intention is lost on the AI:** Often, we include information with a specific intention in mind that seems obvious to us as humans. But if we don’t articulate that intention, the AI won’t magically intuit it. *For instance, a report writer attaches a well-written past report solely to demonstrate the desired writing style for a new report, but doesn’t clarify this intent. Naturally, the AI assumes every attachment is meant to provide content. It ends up borrowing actual passages from the old report when drafting the new one. The writer is dismayed to find the new report filled with last year’s information—the exact situation they were trying to avoid by only wanting the style.* 

From the AI’s perspective, it treats all provided text as potentially relevant unless instructed otherwise. Your internal reasoning—“I’m giving you this document for style, not substance”—stays internal, and the AI proceeds with a different assumption. This gap between your intention and the AI’s interpretation is where things go wrong.

All these challenges boil down to one issue: lack of clarity. When the relevance of each input isn’t specified, the model is unguided. It might err by commission (adding extraneous information) or by omission (missing what really matters). Either way, you don’t get the result you wanted, and you may not even immediately understand why it went awry. Many users initially react by thinking the AI is “not smart enough” or “got confused,” but in reality the prompt itself was confusing. The model can only work with what you give it. If what you give is a big bundle of content without a map, the model will try its best to navigate—but it may take the wrong turn.

It’s also worth noting the efficiency aspect. Large Language Models have a limited attention span. If the AI spends its precious attention parsing irrelevant text, that’s wasted capacity. You could hit context limits or get shorter, less detailed answers because the model’s “mind” was cluttered with unnecessary material. In contrast, when you clarify what to focus on, you effectively direct the model’s mental energy to the right place. Neglecting relevance is like giving someone ten books to read without telling them which chapters contain the answers you need – they might read a lot but still miss the target.

From the user’s perspective, not clarifying relevance can lead to frustration and extra work. You might have to go back and forth refining your prompt, trying to figure out why the AI “doesn’t get it.” Oftentimes, the issue isn’t with the AI’s capabilities but with the prompt’s guidance. I’ve seen colleagues blame the model for being “dumb” when it included a stray fact in its answer, only to realize later that the fact came from an attachment they included without instructions. In a sense, the AI was too obedient—using everything it was given. The failure was ours for not telling it what *not* to use.

Even in a multi-turn conversation, the Relevance principle still applies. Whenever you introduce a new piece of information or switch to a different source mid-chat, it’s wise to briefly note why you’re bringing it in. For example, if partway through a discussion you share a document, add a sentence explaining its purpose (e.g., “This report provides last year’s data for comparison,” or “I’m mentioning this case study because it illustrates our current problem”). By doing so at each step, the AI stays oriented and you avoid accumulating confusion over the course of the session.

Finally, consider the trust factor. If you’re using the AI to help draft a document or analysis, you need to trust that it’s using your sources correctly. When you don’t specify relevance, you might end up questioning the output: *“Where did it get this from? Did I accidentally feed it something misleading?”* That uncertainty can undermine your confidence in the AI’s assistance. The whole point of patterns like **Attachment** and **Prompt File** is to give you control and clarity in how you provide information. The **Relevance** pattern completes that picture by ensuring the purpose of each piece of information is crystal clear. Without it, you’re handing the AI a puzzle with pieces that might not fit — and expecting it to magically see the picture you had in mind.

## Example
To see how clarifying relevance can make or break a prompt, let me share a personal experience. Not long ago, I was drafting an internal report with the assistance of an LLM. I had last year’s report on the same topic, and I wanted this year’s report to have a similar tone and structure. Following the **Attachment** pattern, I dutifully attached the previous year’s report as a reference file. In my main prompt (written in a separate file, per the **Prompt File** pattern), I wrote a brief instruction like, “Use the attached report as a reference in preparing this year’s report.” I then asked the AI to generate the new report.

When the AI’s draft came back, I was in for a surprise. The content of the new report was strangely familiar—too familiar. The AI had indeed imitated the style of the old report, but it had also carried over large chunks of the old report’s content. Some of the 2025 report’s text was essentially copied from the 2024 report I had attached. It started describing last year’s initiatives as if they were this year’s. *For instance, the draft’s introduction was almost identical to the 2024 report’s introduction—repeating details that only applied to the previous year. It even included a section on a project that had been completed in 2024, which had no place in the 2025 report.* In fact, I had provided the AI with a separate list of 2025 achievements to include, but the model largely ignored it—favoring the text from the old report since I hadn’t specified otherwise. My brand-new report was mixing in outdated information from the attachment.

At first, I was confused and a bit disappointed. I wondered, *Why is the model talking about last year’s data?* I had included that document only to guide the writing style, not the substance. Then it hit me: I never actually told the AI my intention. I had said “use it as a reference,” but that was too vague. From the model’s perspective, I’d given it a comprehensive report and asked for a new one—so it figured the safest thing was to reuse a lot of the provided material. The AI assumed every word in that attachment might be relevant to the task. After all, I hadn’t warned it otherwise. By failing to clarify the relevance of the old report, I had practically invited the model to cannibalize its content.

Realizing my mistake, I took a step back and revised my approach. I went back to my prompt file and made a few key changes before running it again. This time, I was very explicit about why I was providing the old report. I added a section in the prompt file (under a clearly labeled “Attachments” heading) and wrote something along these lines for the attached file:

- **Attachment: LastYearReport.pdf** – *This is last year’s internal report on the project. **Use it only as a guide for writing style and format. Do not use its actual content** for this year’s report, as the facts and figures have changed.*

Right after listing this attachment and its purpose, I adjusted the main instruction to the AI. I wrote: *“Please draft the 2025 internal report for our department. The attached 2024 report is provided **only** as an example of tone, style, and structure. Do **not** repeat or copy its content. Instead, emulate its professional tone and organization while using the new data and events from 2025 that I’ll provide below.”* I then supplied the fresh data for 2025 that needed to be in the report.

The difference in output was like night and day. On the second attempt, the AI produced a 2025 report that was written in the same polished, formal style as the 2024 report, but all the content was new and specific to 2025. No longer did outdated initiatives or figures appear in the text. The model followed my instructions: it borrowed the old report’s voice, not its substance. *For example, the first draft had opened with a line taken straight from the 2024 report (“In 2024, our department…”), whereas the revised draft appropriately began with “In 2025, our department…”.* In the new version, the facts were updated while the tone remained expertly consistent with the prior style. I got exactly what I wanted—a well-written report in the right style, with the correct up-to-date information.

This experience drove home the lesson that simply attaching a file isn’t enough; I must also explain why it’s there. I had correctly used the tools at my disposal (the **Attachment** pattern allowed me to provide the old report easily, and the **Prompt File** pattern let me carefully compose my instructions), but I initially neglected to apply the Relevance principle. The attachment by itself was just raw material, and the prompt file by itself was just text. It was the clarification of relevance that tied them together into an effective prompt.

Since that incident, I consistently make a point to spell out the role of every attachment or extra piece of information I include. For example, if I’m drafting a letter and include a sample letter to illustrate the desired tone, I’ll write a note like: “Attachment: SampleLetter – use this as a style guide, do not use its content.” If I attach a spreadsheet of data for a report, I’ll add: “The attached spreadsheet contains data to use for the analysis below.” These brief guideposts only take a few extra seconds to write, and they prevent a world of confusion for the AI.

The value of combining the **Prompt File**, **Attachment**, and **Relevance** patterns is now clear to me. The Prompt File gives me the space and structure to lay out a complex prompt. The Attachment pattern lets me feed in extensive reference material that the AI can use. And the Relevance pattern ensures that for every bit of material I include, the AI knows exactly why it’s there. When these patterns work in concert, using an LLM becomes a much smoother, more predictable process. The AI behaves more like a well-informed assistant following instructions, and less like a perplexed machine trying to read my mind.

In summary, the Relevance pattern is about being a good communicator when you prompt. Just as you would clarify instructions to a human colleague—“I’m giving you this document as background, but focus on the executive summary for what we need”—you should do the same for your AI collaborator. It’s a simple habit that can dramatically improve the quality of the outputs. By always clarifying how each input is relevant to the task at hand (and explicitly saying when something is *not* relevant), you ensure that the AI’s attention stays where it should. In the end, you’ll spend less time correcting the AI’s misunderstandings and more time benefiting from its insights, which is exactly the outcome we’re aiming for with effective prompt design.
