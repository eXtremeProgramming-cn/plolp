# Style Mimicry

*Give the LLM style guides and samples to mimic style and structure.*

## Motivation

Achieving a very specific tone or format in an AI-generated output is notoriously difficult if the model isn’t guided by examples. The large language model (LLM) you’re using has a kind of “default” writing style it falls back on, which might not match what you need. It could be less formal than you want, organized differently, or otherwise inconsistent with the particular style or template you’re aiming for. You might have noticed that even when you ask the LLM to _“write in a formal tone”_ or _“sound like X”_, the results can still feel off. Without **showing** the model what you want, the subtle nuances of your desired style may be lost, leaving you with output that requires a lot of tweaking to get right.

## Solution

The key to controlling the style of the LLM’s output is to provide concrete examples of the style you’re looking for, then explicitly tell the model to follow those examples. In other words, **show, don’t just tell**. You can do this in a few ways:

- **Reference Example Documents:** Attach or provide a sample document that exemplifies the style and structure you want. Then instruct the LLM to use that document as a guide. For instance, you might say: “Refer to the attached report for style — follow its tone and format, but ignore its actual content.” By giving the model a real example, you anchor its output in the characteristics of that example.  
- **Format Imitation Instructions:** Even without a full document, you can describe specific format elements you want the model to mimic. For example: “Use the same section headings and bullet point style as in our company memo template.” This tells the LLM about the structural features of the target style.  
- **Style Snippets:** Include a short excerpt that captures the voice or tone you want, and ask the model to continue in that same style. A few sentences from a well-written piece can serve as a style template. The model will pick up on the vocabulary, tone, and rhythm from that snippet and mirror it in its own writing.

These approaches are not mutually exclusive—you can combine them. Often, providing both a high-level style guide _and_ a bit of example text yields the best results. The core idea is that instead of relying on the LLM to invent the style from a vague description, you give it a pattern to imitate.

## Challenge

**Why style is hard to nail down.** First, style is often implicit and hard to describe in simple instructions. Even skilled writers struggle to pinpoint what makes their writing unique. A friend of mine, Vijay Prashad, is a brilliant author with a very distinctive voice. He once confessed that he finds his writing style “mysterious” and impossible to replicate deliberately—it just flows naturally for him. I gently argued that if we broke down a few of his essays, we’d probably find consistent patterns: recurring turns of phrase, a typical rhythm to his sentences, a certain structure to how he makes his arguments. In other words, I believe that with careful analysis, it’s quite possible to **capture and replicate almost any writer’s style**. But Vijay’s perspective underscores a key point: if even an author of his caliber can’t fully articulate the secret sauce of his style, imagine trying to prompt an AI by simply saying “write like Vijay Prashad.” The AI doesn’t have an intuitive sense of what that means beyond perhaps having seen some of his work during training. Without more guidance, it will likely produce an approximation that doesn’t truly capture Vijay’s flair.

This is a common scenario: not every good writer (or organization) can spell out the elements of their own style. We often know good writing when we see it, but describing it in abstract terms is tricky. One of my colleagues, for example, has a knack for writing engaging policy reports. When he tried to delegate some writing to an assistant, all he could say was, “Please make it concise and punchy like I do.” Not surprisingly, the result didn’t match his style — the assistant wasn’t a mind-reader and produced something either too wordy or too dry. The lesson here is that simply telling someone (or an AI) to “write like me” isn’t effective if you can’t show them what “like me” really entails.

**The limits of “just telling.”** Simply instructing the model _“Use an academic tone”_ or _“be very formal”_ often isn’t enough to guarantee the output matches your expectations. Users frequently find that the results of such generic style instructions feel bland or slightly off-target. For instance, a civil servant in the education sector might prompt the AI to “write a policy brief in a formal government style.” The result may come back grammatically correct and somewhat formal, yet the tone might still be different from the established style of briefs in that department. Perhaps it uses language that is technically polite but not as direct and succinct as their internal guidelines demand. Or maybe it organizes the points in an order that a seasoned official would find odd. The core information might be there, but the presentation doesn’t quite fit the mold. As a result, the person has to spend extra time revising the text, adjusting the wording and structure to meet those unspoken style criteria.

Another example: I once observed a company asking an AI to “draft a press release in our corporate voice” without providing any examples. The AI certainly knew what a press release should look like in general, but it had no specific knowledge of this company’s unique voice. The draft it produced was generic—it lacked the company’s usual taglines, and the tone wasn’t quite the friendly-but-professional vibe their readers were used to. In short, “our corporate voice” was too abstract an instruction. The AI did its best with general corporate-speak, but the company’s communications team still had to heavily edit the draft to insert their brand’s personality back in. This kind of misses the point of using the AI for a first draft—if you have to rewrite a lot of it, you haven’t saved much time.

**Style is surprisingly nuanced.** There are many subtleties that make up a writing style. It’s not just about using formal language or throwing in a few key phrases. Consider things like sentence length variation, preferred vocabulary, the level of detail or context given, how arguments are structured, and even the rhythm or cadence of the prose. These are hard to convey in a quick prompt. For example, your organization might have a habit of starting each report with a single-sentence overview of the situation, followed by three bullet points of key findings in bold. That’s a very specific style convention. A human colleague who reads a few of your reports will likely catch on to that pattern. But an AI won’t know about it unless it either saw many such reports in its training data (unlikely if they’re internal) or you explicitly show it an example. If you just say “write a report in our style,” the AI won’t magically include those bullet points or that one-liner summary, because you haven’t shown it that those features exist.

Let’s look at another subtlety: tone within formality. You might specify “formal tone,” but formal can mean different things in different contexts. In one setting, formal might mean very impersonal and bureaucratic; in another, it might mean polite yet personable. Nuance could also include things like how to refer to the audience (“the reader” vs. “we” vs. “one”), or whether to include certain niceties. Maybe every memo in your office always starts by acknowledging the recipient’s position (e.g., “Esteemed Colleagues, ...”) — a small detail of style that wouldn’t be obvious to an AI from a generic instruction. Or imagine a research summary where the preferred practice is never to write in the first person “I,” whereas another team’s style might welcome a personal touch. Without providing examples, an AI might violate these subtle norms. It might even do so inconsistently — one time formal in one way and next time in another — because these nuances aren’t firmly established in the prompt.

**Consistency across outputs.** Another challenge arises when you need the LLM to produce multiple pieces of writing over time, all in the same vein. It’s one thing to get a single document sounding right after a few tries. But what if you want every monthly report or every press release to have a consistent voice as if they were written by the same person or adhere to the same company style guide? If you rely only on ad-hoc instructions, you might get variability. One day the AI’s output is slightly more stiff, another day a bit more chatty — perhaps because you phrased the prompt differently or the model’s natural randomness introduced small deviations. Ensuring consistency is important for professionalism and branding. Think of how newspapers have an internal style guide to maintain the same voice across different writers; or how a school’s communications might always carry a similar tone regardless of who actually writes them. In the same way, you’ll want the AI to stick to a consistent style guideline so that all outputs feel coherent.

For example, a teacher used an LLM to help draft weekly newsletters for parents. Initially, some weeks she would just instruct, “Write this week’s class newsletter in a friendly tone,” and other weeks she’d paste in a paragraph from an old newsletter as a model. The weeks where she provided the model excerpt, the AI’s output closely matched her usual upbeat, warm style. The weeks where she only gave the generic instruction, the tone drifted — one letter came out overly formal and stiff, and another swung too casual and colloquial. Parents noticed the inconsistency. The teacher realized that for the AI to maintain the voice she wanted, she had to give it the same kind of guidance every time. With a style example or guide included each round, the newsletters became consistently on-brand and on-tone. This illustrates how establishing a stable style input leads to uniform output, whereas just winging it each time can result in a patchwork of styles.

**When the model doesn’t know the style.** If the style you want is uncommon or very specific to you or your group, the model probably wasn’t trained on much (or any) data in that style. Large language models learn from vast amounts of text, but if your desired style is niche, there’s no guarantee the model has a good representation of it. It might try to approximate based on something it thinks is close, often with mixed results. I recall an entrepreneur who attempted to have an AI assistant write his weekly updates in his own quirky way. He prompted the system with, “Draft an update in my voice — casual and witty.” The AI’s version of casual and witty was pretty generic; it certainly didn’t sound like him. It had no idea what his “voice” was — how could it, without being given samples? The output ended up reading like a run-of-the-mill blog post, lacking the specific humor and personal touches the entrepreneur usually included.

Now, consider an even more niche scenario: an internal strategy document for a non-profit, written in the style the board of directors expects. Say this organization has a distinctive way of framing challenges and opportunities, maybe with a bit of inspirational tone mixed with data-driven arguments. The AI won’t inherently produce that style if it has never encountered it. In one case, a team tried to get an AI to draft such a strategy memo. The first attempt, using only general instructions, felt flat and off-key. It read like a standard business memo, missing the motivational spark and the particular phrasing the team was looking for. The model just didn’t know the style. Only after the team provided a couple of past memos as examples did the AI start to mimic the right tone — referencing achievements in a proud manner and using the organization’s familiar slogans appropriately. This turnaround happened because the AI was finally shown what the desired style looked like; before that, it was essentially guessing.

All these challenges boil down to a simple truth: **it’s hard to hit a target you can’t clearly see or describe.** When you can’t easily enumerate what makes a style “tick,” and the AI can’t infer it from a brief prompt, you end up playing the role of editor after the fact. That defeats the purpose of using the AI to lighten your workload — nobody wants to generate text in seconds only to spend an hour tweaking it. It can be frustrating and time-consuming to have to reshape an almost-right-but-not-quite output into the form you wanted.

This is why Style Mimicry is so powerful: it shifts the burden of defining the style from you to the examples you provide. By showing the AI **exactly** what you want the writing to sound like, you remove the guesswork. Instead of hoping the model magically intuits the tone you imagine, you give it concrete guidance. The difference in outcome is like night and day. An LLM’s response that’s guided by actual style examples or a clear style template will have the polish and character you were looking for from the start. In turn, you get to skip the tedious editing phase and move on to your next task, confident that the output is in the right voice. For many users, especially those who aren’t tech experts, this feels empowering — finally the AI isn’t just generating generic text, it’s speaking your language.

## Example

Let’s walk through a real-world application of **Style Mimicry** step by step. Imagine you have to write a type of document that has a very specific style. For instance, I often work with something called _“internal reference”_ reports (known as **内参, _neican_** in Chinese). These are confidential briefing documents prepared for a small group of government leaders. They have a strict format and tone: highly formal, analytical, and concise, with a clear structure of introduction, analysis, and recommendations. Now, suppose I need to draft a new internal reference report on a recent geopolitical development. If I just tell the LLM “write a formal brief about this issue,” I’ll get a formal-sounding output, but it probably won’t meet the nuanced expectations of an actual 内参 document. It might be too wordy, or miss the structured sections that these reports usually have.

To get it right, I decide to use Style Mimicry. I have two excellent examples of past internal reference reports on hand (from earlier projects). My plan is to show those to the LLM so it knows exactly the style I’m aiming for. Here’s how I proceed:

**1. Providing reference documents.** I take the two previous 内参 documents and provide them to the LLM as reference material (using the "Attachment" technique described in another pattern). Essentially, I feed the model these examples in full. Now the model isn’t flying blind—it has concrete samples of the kind of output I want. These samples illustrate everything: the formality, the phrasing, the way the content is organized into sections with specific headings, and even little details like how sources are cited at the end.

**2. Prompting the model to analyze the style.** Rather than immediately asking for a new report, I first want to make sure the model understands the style deeply. So I give it an intermediate task: _style analysis_. I write a prompt along the lines of:

```
I have attached two internal reference reports. Please analyze these documents and identify the common structure and writing style. Summarize the key characteristics in a brief style guide that I could use to write new reports in the same style.
```

In essence, I’m asking the LLM to reverse-engineer the pattern from the examples. This step is leveraging another technique (sometimes called **Deconstruction** or reverse prompting) which is related to Style Mimicry. I’m explicitly telling the model to study the examples and tell me what makes them tick.

**3. The model produces a style guide.** The AI goes through the two example documents and comes back with a remarkably detailed “writing guide.” The output it gives me is essentially a template describing how to write an internal reference. For instance, it starts by noting the **Purpose and Audience** of these reports: it says something like _“Internal reference documents provide timely, concise, actionable analysis for high-level officials; they assume the reader is knowledgeable and very busy.”_ This is spot-on – it captured why the documents exist and who they’re for, which is crucial context for understanding the tone.

Then the guide breaks down **Key Structural Components** of the reports. It outlines that there should be a clear **Title** that highlights the issue and urgency. It describes the **Introduction** as a brief context-setter that states the problem and its importance. It notes that the **Body** of the report is usually divided into numbered sections (perhaps “1, 2, 3” or in Chinese “一, 二, 三”) – the first section describes the situation or problem in detail, the second analyzes the causes or background, and the third assesses the implications or risks. It even highlights that a common section might be **Risk Assessment / Implications** for the organization (in this case, implications for national interests, since these were Chinese reports), followed by a section of **Recommendations** or **Countermeasures**. The guide mentions that a **Conclusion**, if present, is brief or sometimes omitted, because often the recommendations serve as the conclusion.

What’s impressive is that the AI picked up not just on the outline, but also on the **writing style and tone**. The style guide it generated explicitly says things like: _“Use formal, standard language; avoid colloquialisms. The tone should be direct and analytical, focusing on facts and cause-and-effect. Maintain an objective stance (within the perspective of our organization’s interests). Be concise — respect the reader’s time. If the issue is urgent, convey urgency with words that stress immediate attention.”_ In fact, it even listed some example phrases it noticed in the documents to convey urgency (for instance, phrases equivalent to “needs attention” or “urgent”). It basically codified the tone: formal, direct, analytical, and when appropriate, urgent and proactive.

The style guide also included a section on **Things to Avoid** – which I hadn’t even explicitly considered beforehand. It cautioned against vagueness or unsupported claims (so any assertion should be backed by evidence or reasoning). It said not to use overly flowery language or obscure jargon that busy policymakers would not appreciate. It emphasized avoiding analysis without clear implications or recommendations. Reading this guide, I realized the model had distilled the essence of these documents better than I could have if I tried to write down the style rules myself!

At this point, I have a custom-made summary of the style I need – a tailored guide. In the context of this book’s patterns, I effectively got the model to produce a "Chapter" of instructions that I can reuse. I saved this guide for future reference. It was a few pages long (around 800 words) and covered everything from structure to tone to formatting. It’s like having a cheat-sheet for writing internal references.

**4. Applying the style guide to a new task.** Now comes the real payoff. I ask the LLM to draft a new internal reference report on my topic of interest (say, a new international event that I need to cover), but this time I include the style guide in my prompt. There are a couple of ways to do this. One way is literally appending the style guide text above my request, essentially saying, “Here are the guidelines to follow. Now, using these, write about **[the new topic].**” Another way, if the system allows, is to attach the style guide as a reference (similar to how I attached the original examples) and instruct the model: “Follow the writing guidelines in the attached style guide when drafting the report.” Either way, the model now has very explicit directions on how to format and phrase the output.

When the LLM produces the draft, the difference is night and day compared to a generic attempt. The output follows the template: it comes back with a clear title that matches the expected format, an introduction that is just a few sentences long and hits the key points, and then neatly structured sections 1, 2, 3 as outlined. The tone throughout the draft is formal and authoritative. It reads very much like the earlier internal references – in fact, if you didn’t know, you might think the same person who wrote the previous reports wrote this one as well. The content, of course, is new (it’s about the latest topic I asked it to cover), but the way it’s written now mirrors the established style perfectly. It’s concise where it needs to be, it conveys urgency in the right spots (using phrases the style guide recommended), and the recommendations at the end are phrased as clear, actionable suggestions directed to the relevant officials, just as the style guide specified.

Importantly, I find that I have to do far less editing. Previously, if I had just gotten a raw draft from the AI with minimal instruction, I might have had to rearrange sections, rewrite the intro, tone down or amp up certain language, or insert phrasing that fits the 内参 style. Now, because the model was working from the style guide, it already avoided the common pitfalls. For example, one issue I saw before was that the AI might write too lengthy a background section and bury the key point. But guided by the style instructions (“be concise, assume the reader is knowledgeable”), the draft’s background section is short and efficient. It spends more time on analysis and recommendations, which is exactly right for this kind of document. In short, the heavy lifting of conforming to the style was handled by the AI during generation, not by me afterward.

**5. Generalizing to other contexts.** While this example was about a specific kind of report, the same pattern can help in countless scenarios. The core steps are: pick example texts that represent your desired style, let the model learn from them, and then provide that guidance whenever you generate new text. If you are a civil servant or professional who has to produce writing in a consistent format (be it policy briefs, press releases, classroom lesson plans, or any document with a standard style), you can create your own style guide with the help of the AI. 

Consider an educator who regularly writes official letters to parents in a very particular tone – friendly but formal, encouraging yet authoritative. Suppose he has a few great letters he wrote in the past. By giving those to the LLM and asking for a style analysis, he might get back a useful summary like: “Tone: warm and respectful, avoids jargon; uses simple sentences in the greetings. Structure: always starts with a positive note, then addresses the issue, and ends with an offer of further communication.” He could then use that guide whenever he asks the AI to draft a new letter, ensuring the draft maintains that same voice and structure that he knows works well.

Or think of a marketing team using an LLM to draft product descriptions. They want each description to match the company’s brand voice. They could feed the model a handful of existing product descriptions that perfectly capture the voice, and either directly prompt the model to mimic those or first extract a guide (“Our brand voice is playful yet professional, uses upbeat language, and always highlights how the product makes life easier for the customer,” etc.). Going forward, every time the AI writes a new description, the team includes those style notes or an example in the prompt. The result is a uniform style across all content, even if different team members are generating it at different times.

The beauty of Style Mimicry is that it turns the elusive art of style into something a bit more concrete for the AI. You don’t have to hope the model has implicitly learned your style from its training data – you show it exactly what you mean by “our style” right there in the prompt. And you don’t have to be a professional writer or tech expert to do this: it’s often as simple as saying, “Here’s what I want it to sound like – now do the same.”

**Related Patterns:** In practice, Style Mimicry often works hand-in-hand with other prompting techniques. The "Attachment" pattern is crucial because it allows you to feed lengthy example documents or style guides into the model as context. The "Example" pattern (providing examples of the task in the prompt) is a closely related idea – indeed, Style Mimicry is a special case of using examples, focused on tone and format. We also saw a bit of the "Chapter" pattern in action when we created a standalone style guide section that can be reused in prompts; treating that guide as a modular piece of your prompt (like a chapter in a manual) helps keep things organized and consistent. By combining these patterns, even a novice user can systematically guide an LLM to produce text that not only has the right information, but also _sounds and reads just right_.
